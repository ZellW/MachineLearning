---
title: 'Auto Val Unbalanced Data Experimentation'
output:
  rmdformats::readthedown:
    highlight: pygments
    code_folding: hide
---
<style type="text/css">
p{ /* Normal  */
   font-size: 14px;
   line-height: 18px;}
body{ /* Normal  */
   font-size: 14px;}
td {  /* Table  */
   font-size: 12px;}
h1 { /* Header 1 */
 font-size: 26px;
 color: #4294ce;}
h2 { /* Header 2 */
 font-size: 22px;}
h3 { /* Header 3 */
 font-size: 18px;}
code.r{ /* Code block */
  font-size: 12px;}
pre { /* Code block */
  font-size: 12px}
#table-of-contents h2 {
  background-color: #4294ce;}
#table-of-contents{
  background: #688FAD;}
#nav-top span.glyphicon{
  color: #4294ce;}
#postamble{
  background: #4294ce;
  border-top: ;}
</style>

```{r echo=FALSE, warning=T, message=T}
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("recipes", "caret", "rsample", "purrr", "MASS", "tidyposterior", "dplyr", "yardstick", prompt = TRUE)
options(digits = 3)
```

```{r setWD, echo=FALSE}
setwd("~/R/WIP") #change as needed
```

#Introduction

Subsampling can be a helpful approach to dealing will classification data where one or more classes occur very infrequently. Often, most models will overfit to the majority class and produce very good statistics for the class containing the frequently occurring classes while the minority classes have poor performance. 

Consider a two-class problem where the first class has a very low rate of occurrence. The [`caret`](https://topepo.github.io/caret/) package has a function that can simulate such data:

```{r simulate}
imbal_data <- twoClassSim(1000, intercept = 10)
table(imbal_data$Class)
```

If "Class1" is the event of interest, it is very likely that a classification model would be able to achieve very good _specificity_ since almost all of the data are the second class. _Sensitivity_ will often be poor since the models will optimize accuracy (or other loss functions) by predicting everything to be the majority class. 

When there are two classes, the results is that the default probability cutoff of 50% is inappropriate; a different cutoff that is more extreme might be able to achieve good performance. 

One way to alleviate this issue is to _subsample_ the data. There are a number of ways to do this but the most simple one is to _sample down_ the majority class data until it occurs with the same frequency as the minority class. While counterintuitive, throwing out a large percentage of the data can be effective at producing a results. In some cases, this means that the overall performance of the model is better (e.g. improved area under the ROC curve). However, subsampling almost always produces models that are _better calibrated_, meaning that the distributions of the class probabilities are model well behaved. As a result, the default 50% cutoff is much model likely to produce better sensitivity and specificity values than they would otherwise. 

To demonstrate this, `step_downsample` will be used in a recipe for the simulated data. In terms of workflow:

> It is extremely important that subsampling occurs _inside of resampling_. Otherwise, the resampling process can produce [poor estimates of model performance](https://topepo.github.io/caret/subsampling-for-class-imbalances.html#resampling). 

The important consideration is that this preprocessing is only applied to the training set so that it can impact the model fit. The test set should be unaffected by this operation. If the recipe is used to create the design matrix for the model, down-sampling would remove rows. This would be a bad idea for the test set since these data should represent what the population of samples looks like "in the wild.". Based on this, a recipe that included `down-sample` should skip this step when data are baked for the test set.

As of version `recipes 0.1.2`, each step has an optional logical argument called skip. In almost every case, the default is TRUE. When using this option:

- No steps are skipped during `prep`
- Steps with `skip = TRUE` are not applied to the data when `bake` is called

Recall that there are two ways of getting the results for the training set with recipes. First, `bake` can be used as usual. Second, `juice` is a shortcut that will use the already processed data that is contained in the recipe when `prep(recipe, retain = TRUE)` is used. `juice` is much faster and would be the way to get the training set with all of the steps applied to the data. For this reason, you should almost always used `retain = TRUE` if any steps are skipped (and a warning is produced otherwise).

------------------
Skipping is a necessary feature but can be dangerous if used carelessly.

As an example, skipping an operation whose variables are used later might be an issue:

```{r}
car_recipe <- recipe(mpg ~ ., data = mtcars) %>%
  step_log(disp, skip = TRUE) %>%
  step_center(all_predictors()) %>%
  prep(training = mtcars, retain = TRUE)
car_recipe %>% prep()
```


These *should* produce the same results (as they do for `hp`)

```{r}
juice(car_recipe) %>% head() %>% select(disp, hp)
bake(car_recipe, newdata = mtcars) %>% head() %>% select(disp, hp)
```

This should emphasize that juice should be used to get the training set values whenever a step is skipped.
------------------

Here is a simple recipe: 

```{r rec}
imbal_rec <- recipe(Class ~ ., data = imbal_data) %>%  step_downsample(Class)
imbal_rec %>% prep()
```

Basic cross-validation is used to resample the model:

```{r cv}
set.seed(5732)
cv_folds <- vfold_cv(imbal_data, strata = "Class", repeats = 5)
```

An additional column is added to the data that contains the trained recipes for each resample:

```{r prep}
library(purrr)
cv_folds <- cv_folds %>% mutate(recipes = map(cv_folds$splits, prepper, recipe = imbal_rec, retain = TRUE))
cv_folds$recipes[[1]]
```

The model that will be used to demonstrate subsampling is [quadratic discriminant analysis](https://en.wikipedia.org/wiki/Quadratic_classifier#Quadratic_discriminant_analysis) via the `MASS` package. A function will be used to train the model and to produce class probabilities as well as hard class predictions using the default 50% cutoff. When a recipe is passed to the function, down-sampling will be applied. If no recipe is given, the data are used to fit the model as-is:

```{r func}
assess_res <- function(split, rec = NULL, ...) {
  if (!is.null(rec))
    mod_data <- juice(rec)
  else
    mod_data <- analysis(split)
  
  mod_fit <- qda(Class ~ ., data = mod_data)
  
  if (!is.null(rec))
    eval_data <- bake(rec, assessment(split))
  else
    eval_data <- assessment(split)
  
  eval_data <- eval_data 
  predictions <- predict(mod_fit, eval_data)
  eval_data %>%
    mutate(
      pred = predictions$class,
      prob = predictions$posterior[,1]
    ) %>%
    dplyr::select(Class, pred, prob)
}
```

## Example: No Sampling

```{r ex}
assess_res(cv_folds$splits[[1]]) %>% head
```

## Example:  With Downsampling

```{r}
assess_res(cv_folds$splits[[1]], cv_folds$recipes[[1]]) %>% head
```

To measure model effectiveness, two metrics are used:

 * The area under the [ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) is an overall assessment of performance across _all_ cutoffs. Values near one indicate very good results while values near 0.05 would imply that the model is very poor. 
 * The _J_ index (a.k.a. [Youden's _J_](https://en.wikipedia.org/wiki/Youden%27s_J_statistic) statistic) is `sensitivity + specificity - 1`. Values near one are once again best. 

If a model is poorly calibrated, the ROC curve value might not show diminished performance. However, the _J_ index would be lower for models with pathological distributions for the class probabilities. The `yardstick` package will be used to compute these metrics. 

Now, we train the models and generate the predictions. These are stored in list columns where each list element is a data frame of the predictions on the assessment data:

```{r fits}
cv_folds <- cv_folds %>%
  mutate(
    sampled_pred = map2(cv_folds$splits, cv_folds$recipes, assess_res),
    normal_pred = map(cv_folds$splits, assess_res)
  )
cv_folds
```

Next performance metrics are computed: 

```{r perf}
library(yardstick)
cv_folds <- cv_folds %>%
  mutate(
    sampled_roc = map_dbl(cv_folds$sampled_pred, roc_auc, truth = "Class", estimate = "prob"),
    normal_roc =  map_dbl(cv_folds$normal_pred,  roc_auc, truth = "Class", estimate = "prob"),  
    sampled_J =   map_dbl(cv_folds$sampled_pred, j_index, truth = "Class", estimate = "pred"),
    normal_J =    map_dbl(cv_folds$normal_pred,  j_index, truth = "Class", estimate = "pred")       
  )
```

What do the ROC values look like? A [Bland-Altman plot](https://en.wikipedia.org/wiki/Bland%E2%80%93Altman_plot) can be used to show the differences in the results over the range of results:

```{r bland-altman-roc}
library(ggplot2)

ggplot(cv_folds, aes(x = (sampled_roc + normal_roc)/2, y = sampled_roc - normal_roc)) + 
  geom_point() + geom_hline(yintercept = 0, col = "green")
```

There doesn't appear that subsampling had much of an effect on this metric. The average difference is `r signif(mean(cv_folds$sampled_roc - cv_folds$normal_roc), 3)`, which is fairly small. 

For the _J_ statistic, the results show a different story: 

```{r bland-altman-j}
ggplot(cv_folds, aes(x = (sampled_J + normal_J)/2, y = sampled_J - normal_J)) + 
  geom_point() + geom_hline(yintercept = 0, col = "green")
```

Almost all of the differences area greater than zero. We can use `tidyposterior` to do a more formal analysis:

```{r tpost, warning = FALSE, message=FALSE}
# Remove all columns except the resample info and the J indices,
# then fit the Bayesian model
j_mod <- cv_folds %>% dplyr::select(-recipes, -matches("pred$"), -matches("roc$")) %>% perf_mod(seed = 62378, iter = 5000)
```

A simple plot of the posterior distributions of the _J_ indices for each model shows that there is a real difference; subsampling the data prior to modeling produced better calibrated models:

```{r post-plot}
ggplot(tidy(j_mod, seed = 234))
```



