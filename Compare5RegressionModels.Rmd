---
title: 'Compare 5 Regression Models - Remarkable Plots!'
output:
    rmdformats::readthedown:
      highlight: pygments
      code_folding: hide
---

<style type="text/css">
p{ /* Normal  */
   font-size: 14px;
   line-height: 18px;
}
body{ /* Normal  */
   font-size: 14px;
}
td {  /* Table  */
   font-size: 12px;
}
h1 { /* Header 1 */
font-size: 26px;
color: #4294ce;
}
h2 { /* Header 2 */
font-size: 22px;
}
h3 { /* Header 3 */
font-size: 18px;
}
code.r{ /* Code block */
  font-size: 12px;
}
pre { /* Code block */
  font-size: 12px
}
#table-of-contents h2 {
background-color: #4294ce;
}
#table-of-contents{
background: #688FAD;
}
#nav-top span.glyphicon{
color: #4294ce;
}
#postamble{
background: #4294ce;
border-top: ;
}
</style>

```{r loadLibs1, warning=FALSE, message=FALSE}
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("tidyverse", "ggthemes", "caret", "rpart","rpart.plot", "modelr", "gridExtra", "lubridate", 
         "broom", "ranger", "Cubist", "gbm", prompt = FALSE)
load("~/GitHub/LargeDataFiles/Compare5RegressionModels.RData")
```

# Objectives

We’ll explore five approaches to supervised machine learning. The objective is quantitative prediction. So, the first modelling choices we’ll make are to use regression, rather than classification, and include more flexible models at the expense of interpretability. And as we’re dealing with count data, we’ll use a Poisson distribution.

We’ll compare the outcomes of the following models:

- Random Forest using ranger
- Generalized Linear Model using glm
- CART using rpart
- Stochastic Gradient Boosting using gbm
- Cubist using cubist

# Import the data

We’ll use recorded crime summary data at London borough-level from [data.gov.uk](https://files.datapress.com/london/dataset/recorded-crime-summary-data-london-borough-level/2017-01-26T18:50:00/MPS_Borough_Level_Crime.csv).

```{r eval=FALSE}
crime_df <-
  read_csv("MachineLearning/Models/data/MPS_Borough_Level_Crime.csv", col_types = "cccci",
    col_names = c("month", "borough", "maj_cat", "min_cat", "count"), skip = 1) %>%
  separate(month, c("year", "month"), sep = 4) %>%
  mutate(year = factor(year), month = factor(month), maj_cat = factor(maj_cat), borough = factor(borough)) %>%
  group_by(year, month, borough, maj_cat) %>% summarise(count = sum(count))
```

# Explore the Data

## Visualize the Detail

Next we’ll get a sense of the data by plotting the trends for the 9 major crime categories in the 32 London boroughs. The dataset covers the period 2013/11 to 2017/01.

The data have multiple potential explanatory variables (i.e. input, predictor, independent or X variables), and a single response (i.e. output, dependent or Y variable).

```{r eval=FALSE}
cap <- labs(caption = paste0(
  "\nSource: data.gov.uk (", min(str_c(crime_df$year, "/", crime_df$month)), " to ",
  max(str_c(crime_df$year, "/", crime_df$month)), ")\nData Science Powered by R"
  ))
```

```{r ExploreDetail, fig.align='center', fig.height=20, fig.width=9}
ggplot(crime_df, aes(str_c(year, "/", str_pad(month, 2, pad = 0)), count, color = maj_cat, group = maj_cat)) +
  geom_line() + facet_wrap( ~ borough, scales = "free_y", ncol = 4) + theme_economist() +
  scale_colour_economist(name = "Major Category") +
  scale_x_discrete(breaks = c("2013/12", "2014/12", "2015/12", "2016/12", "2017/12")) +
  ggtitle("London Crime by Borough\n") + labs(x = NULL, y = NULL, caption = cap) +
  theme(
    rect = element_rect(fill = "#f9f5f1"), plot.background = element_rect(fill = "#f9f5f1"),
    text = element_text(size = 7), strip.text = element_text(size = 8),
    axis.text.x = element_text(angle = 45, vjust = 0.4))
```

## Summarize Variables

The faceted plot above hints at potential interaction between borough and major crime category. In more affluent boroughs, and/or those attracting greater visitor numbers, e.g. Westminster and Kensington & Chelsea, “theft and handling” is the more dominant category. In Lewisham, for example, “violence against the person” exhibits higher counts. So, as well as tuning the models, we’ll assess them both with and without a borough-category interaction term.

Before we model and predict crime counts, let’s also visualize the dependent variable against each independent variable. The 9 major crime categories, and 32 boroughs show significant variation in crime counts, and there is also evidence of an increase over time (the first and fifth years are partial years).

```{r}
these <- c("maj_cat", "borough")
crime_df$month <- as.integer(crime_df$month)
```
```{r}
EDA_plot <- function(x) {
  ggplot(crime_df, aes_string(x, "count")) +
  theme_economist() +
  theme(
    rect = element_rect(fill = "#f9f5f1"),
    plot.background = element_rect(fill = "#f9f5f1")) +
  {if (x %in% these) geom_boxplot(fill = "#CFB292")} +
  {if (x %in% these) scale_y_log10()} +
  {if (x %in% these) theme(axis.text.x = element_blank())} +
  {if (x %in% these) theme(axis.title.x = element_text(vjust = -2))} +
  {if (x == "month" | x == "borough") theme(axis.title.y = element_blank())} +
  {if (x == "month") scale_x_continuous(breaks = seq(1, 12, by = 1))} +
  {if (x == "month") coord_cartesian(ylim = c(200, 230))} +
  {if (!x %in% these) geom_smooth(colour = "black")} +
  {if (x == "year") geom_col(fill = "#CFB292")} +
  {if (x == "month") geom_jitter(alpha = 0.6, width = 0.2)}
}
```
```{r message=FALSE}
p <- map(c("maj_cat", "borough", "year", "month"), EDA_plot)
grid.arrange(p[[1]], p[[2]], p[[3]], p[[4]], top = "Exploratory Data Analysis")

crime_df$month <- factor(crime_df$month)
```

# Which Variables Matter?

Before we train our models to predict crime counts, we’ll assess which variables really matter. rpart provides a nice architecture to visually address this question.

At the top of the tree (node 1) we have 100% of the 11053 observations. The first, and most important split, is based on the major crime category: 23% of the observations are partitioned off to the right (to node 3), for “Theft and Handling” (T&H) and “Violence Against the Person” (VATP), with the balance branching left to node 2.

We could go to lower levels of granularity in the diagram, but our purpose here is to assess the most important variables. maj_cat, borough and year feature in these top levels, however, we’ll include month in our modelling formula too to further assess it’s significance.

```{r eval=FALSE}
temp <- rpart(count ~ ., data = crime_df, cp = 0.002)

text_wrap <- function(x, labs, digits, varlen, faclen) {
  labs <- str_replace_all(labs, ",", " ")
  for (i in 1:length(labs)) {
    labs[i] <- str_c(str_wrap(labs[i], 20), "\n")
  }
  labs
}
```

```{r fig.align='center', fig.height=20, fig.width=9, warning=FALSE}
prp(temp, extra = 101, box.palette = "#f9f5f1", type = 4, cex = 0.7, yspace = 4, nn = TRUE, nn.cex = 1.4,
  nn.box.col = "#CFB292", space = 2, split.font = 0.9, branch.lty = 2, faclen = 2, split.fun = text_wrap,
  main = "Which variables matter?")
```

```{r replacePRP, fig.align='center', fig.height=20, fig.width=9, warning=FALSE, eval=FALSE, echo=FALSE}
#academic exercise only b/c prp (above) is the older function
rpart.plot(temp, extra = 101, box.palette = "#f9f5f1", type = 4, cex = 0.7, yspace = 4, nn = TRUE, nn.cex = 1.4,
  nn.box.col = "#CFB292", space = 2, split.font = 0.9, branch.lty = 2, faclen = 2, split.fun = text_wrap,
  main = "Which variables matter?")
```

# Prepare Reusable Objects

## Training Control

We’ll use repeated k-fold cross-validation, a method that works well with models generally irrespective of their fitting procedures. The observations are split into “k” equal folds with one randomly-selected for testing the model. The balance is used for training the model. So, we’ll set this up as our training control so we can reuse these parameters with each of our regression models.

```{r eval=FALSE}
train_control <- trainControl(method = "repeatedcv", number = 10, repeats = 5, allowParallel = TRUE, seeds = c(1:51))
```

## Plot theme for model predictions

We’ll also create a reusable theme for some of the later plots.

```{r}
theme_thinkr <- theme_economist() + theme(
  rect = element_rect(fill = "#f9f5f1"),
  plot.title = element_text(size = 12),
  plot.subtitle = element_text(size = 6),
  strip.text = element_text(size = 9),
  axis.text.x = element_text(size = 7),
  legend.text = element_text(size = 7),
  plot.background = element_rect(fill = "#f9f5f1")
  )
```

# Build models

## Generalized Linear Model

Now we’ll use the glm function with a Poisson distribution for count data. Our formula count ~ . is read as count explained by all other variables in the dataset, i.e. borough, major category, year and month. We’ll also train a second model with the interaction between borough and major category, i.e. count ~ borough * maj_cat + ..

By reviewing the R-squared and RMSE (Root Mean Squared Error) both with, and without, the interaction term, we can see that the model with delivers a materially stronger correlation between predictions and actuals, and lower RMSE. We’ll prioritize RMSE as our key metric for evaluating models, essentially measuring the average residual (prediction error with respect to the actual).

We’ll defer a visual assessment of our Generalized Linear Model until we have the performance of all five regression models to compare side-by-side.

```{r eval=FALSE}
model_GLM <- train(count ~ borough * maj_cat + ., data = crime_df, method = "glm", metric = "RMSE",
                   family = "poisson", trControl = train_control)

model_GLM2 <- train(count ~ ., data = crime_df, method = "glm", metric = "Rsquared", family = "poisson",
                    trControl = train_control)

(compare <- data_frame(
  Statistic = c("R-squared", "RMSE"),
  Without_interaction = c(model_GLM2$results$Rsquared, model_GLM2$results$RMSE),
  With_interaction = c(model_GLM$results$Rsquared, model_GLM$results$RMSE)
  ))
```
```{r}
compare
```

Returning to the significance of month, we see that the glm p-values are close to zero. This confirms it does make sense to include this variable in our models.

```{r}
tidy(model_GLM$finalModel) %>% filter(str_detect(term, "month")) %>% select(term, p.value)
```

Plotting the residuals of the GLM model, we do see an increasing spread as the count rises.

```{r}
gather_residuals(crime_df, model_GLM, .resid = "resid", .model = "model") %>%
  ggplot(aes(count, resid, colour = maj_cat)) + geom_point() +
  ggtitle("GLM residuals spread out at higher counts") +
  geom_hline(yintercept = 20, lty = 2, size = 1) +
  geom_abline(intercept = 80, slope = 0.15, colour = "grey80", size = 2, lty = 3) +
  geom_abline(intercept = -80, slope = -0.17, colour = "grey80", size = 2, lty = 3) +
  scale_colour_economist() + theme_thinkr + cap
```

## Recursive Partitioning

Although we built a recursive partitioning model earlier to select the modeling variables, run it again here inside the `caret train` function. This tunes / refines the model and creates consistent outputs across the five models. Consistency, for example, will enable us to pluck some of the information out of the final models to annotate the concluding plot.

```{r eval=FALSE}
tune_grid <-
  expand.grid(cp = 0.00001)) # tried: 0.001, 0.0001

model_RP <- train(count ~ ., # tried: interaction (negligible benefit)
    data = crime_df, method = "rpart", metric = "RMSE", parms = list(method = "poisson"),
    tuneGrid = tune_grid, trControl = train_control)
```

## Random Forest

Random Forest combines a large number of decision trees by deploying a bagging procedure to repeatedly split the training data horizontally and another procedure to randomly sample the predictors vertically.

```{r eval=FALSE}
tune_grid <- expand.grid(mtry = 18, # 54 variables divided by 3 (allowing for dummy variables)
    splitrule = "variance", min.node.size = 5)

model_RF <- train(count ~ ., # tried: interaction (negligible benefit)
                  data = crime_df, method = "ranger", num.trees = 500, importance = "impurity", metric = "RMSE",
                  respect.unordered.factors = TRUE, tuneGrid = tune_grid, trControl = train_control)

tune_grid <- expand.grid(mtry = 2, splitrule = "variance", min.node.size = 5)

model_RF2 <- train(count ~ ., data = crime_df, method = "ranger", num.trees = 500, metric = "RMSE",
                   respect.unordered.factors = TRUE, tuneGrid = tune_grid, trControl = train_control)
```

At first, we might be tempted to model with an mtry of 2 given the four variables of borough, major category, year and month. However, our categorical variables will actually generate 54 dummy variables, so an mtry of 54 / 3 = 18 would be more appropriate. We can see the impact of this choice below.

```{r}
crime_df %>% 
  spread_predictions("Random Forest | mtry = 18" = model_RF,
                     "Random Forest | mtry = 02" = model_RF2) %>% 
  gather(key = model,
         value = pred,-year,-month,-borough,-maj_cat,-count) %>%
         rename(act = count) %>%
         ggplot(aes(pred, act, colour = maj_cat)) +
         geom_point(alpha = 0.3, size = 2) +
         geom_abline(colour = "black", lty = 2) +
         facet_wrap( ~ model) +
         scale_colour_economist(name = "Major Category") +
         scale_y_continuous(breaks = seq(500, 3000, by = 500),
         limits = c(0, 3000)) +
         scale_x_continuous(breaks = seq(500, 3000, by = 500),
         limits = c(0, 3000)) +
         ggtitle("Accounting for the dummy variables when setting 'mtry'",
         subtitle = "The four categorical variables create 54 independent variables") +
         labs(x = "Predictions", y = "Actual") +
         guides(colour = guide_legend(override.aes = list(size = 3))) +
         theme_thinkr +
         cap
```

## Stochastic Gradient Boosting

Boosting is another way to improve predictions from decision trees. Unlike bagging, whereby we combine trees grown from different cuts of the training data, boosting is more akin to natural selection whereby successive trees are a little fitter than their predecessor.

```{r eval=FALSE}
tune_grid <- expand.grid(interaction.depth = 10, # tried: 8, 9
    n.trees = 500, shrinkage = 0.1, # tried: 0.001, 0.01, 0.05
    n.minobsinnode = 5) # tried: 4

model_SGB <- train(count ~ ., data = crime_df, distribution = "poisson", method = "gbm",
                   metric = "RMSE", tuneGrid = tune_grid, verbose = FALSE,
                   bag.fraction = 0.5, trControl = train_control)
```

##Cubist

Cubist is a rule-based variant of other tree models with boosting controlled by committees and predictions tweaked using neighbouring points.

```{r eval=FALSE}
tune_grid <- expand.grid(committees = 80, # tried 10, 40
    neighbors = 9) # tried 0, 5

model_Cub <- train(count ~ ., data = crime_df, method = "cubist", metric = "RMSE", tuneGrid = tune_grid, trControl = train_control)
```

# Measure Performance

Now we can apply our measure function consistently across the test results of all five approaches. We’ll also gather the five sets of measurements into a little dataset that we can use to augment our final plot.

```{r eval=FALSE}
combined_pred <- crime_df %>% spread_predictions("Generalized Linear Model" = model_GLM, "Random Forest" = model_RF,
                                                 "Stochastic Gradient Boosting" = model_SGB, "CART" = model_RP,
                                                 "Cubist" = model_Cub) %>% 
  gather(key = model, value = pred,-year,-month,-borough,-maj_cat,-count) %>% rename(act = count)

pred_meas <- map_df(list(model_GLM, model_RF, model_SGB, model_RP, model_Cub), 
                    function(x) {tibble(model = x$modelInfo$label, method = x$method, 
                                        model_type = x$modelType, rmse = rmse(x, crime_df), 
                                        rsquare = rsquare(x, crime_df))})
```

# Compare Performance

Finally, we will compare the performance of the five models, both visually and quantitatively.

```{r}
ggplot(combined_pred, aes(pred, act, colour = maj_cat)) + geom_point(alpha = 0.3, size = 2) +
     geom_abline(colour = "black", lty = 2) + facet_wrap(~ model, nrow = 1) +
     geom_text(x = 1900, y = 250, aes(label = paste0("Method = ", subset(method, model %in% combined_pred$model),
                                 "\n", "Type = ", subset(model_type, model %in% combined_pred$model),
                                 "\n", "RMSE = ", round(subset(rmse, model %in% combined_pred$model), 3),
                                 "\n", "R squared = ", round(subset(rsquare, model %in% combined_pred$model), 3))),
               data = pred_meas, color = "black", size = 3) +
     scale_colour_economist(name = "Major Category") + 
     scale_y_continuous(breaks = seq(500, 3000, by = 500), limits = c(0, 3000)) +
     scale_x_continuous(breaks = seq(500, 3000, by = 500), limits = c(0, 3000)) +
     ggtitle("Comparison of 5 Regression Models", subtitle = "Crime by category for each of London's 32 boroughs") +
     labs(x = "Predictions", y = "Actual") + guides(colour = guide_legend(override.aes = list(size = 3))) + theme_thinkr + cap
```

# Variable Importance

We can also plot the top 10 variables in terms of their importance to the final Random Forest model.

```{r eval=FALSE}
var_imp <- varImp(model_RF)$importance %>%
  as.data.frame() %>%
  rownames_to_column() %>%
  arrange(Overall) %>%
  top_n(10) %>%
  mutate(rowname = str_replace(rowname, "borough", "borough | "),
         rowname = str_replace(rowname, "maj_cat", "category | "),
         rowname = fct_inorder(rowname),
         hjust = if_else(Overall > 40, 1.1, -0.3),
         labcol = if_else(Overall > 40, "white", "black"))
```
```{r}
ggplot(var_imp, aes(rowname, Overall)) + geom_col(fill = economist_pal()(1), width = 0.8) +
     geom_text(aes(label = paste0(rowname, "  ", round(Overall, 2))),
               hjust = var_imp$hjust, colour = var_imp$labcol, size = 3) +
     ggtitle("Top 10 Variables by Importance", subtitle = "Scaled to 100 -- Random Forest") +
     scale_y_log10() + coord_flip() + theme_economist() +
  theme(
    rect = element_rect(fill = "#f9f5f1"),
    plot.caption = element_text(colour = "grey60"),
    axis.text.x = element_blank(),
    axis.line.x = element_blank(),
    axis.text.y = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    plot.background = element_rect(fill = "#f9f5f1")) + cap
```

# Conclusion

Our supervised machine learning outcomes from the CART and GLM models have weaker RMSEs, and visually exhibit some dispersion in the predictions at higher counts. Stochastic Gradient Boosting, Cubist and Random Forest have handled the higher counts better as we see from the visually tighter clustering.

Reference:  https://thinkr.biz/2018/03/01/crime-random-forest/