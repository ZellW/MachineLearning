---
title: "Clustering Tendency"
output: html_document
---
# Introduction

We have explored how to perform clustering but a question remains - *Should we?*

A big issue in cluster analysis is clustering methods will alsways return clusters even if the data does not have any meaningful clusters.  If you blindly apply a clustering methid on data, it will divide the data into clusters because that is what it is designed to do.

This document provides guidance to evaluate if the data contains meaningful clusters.  This is call **Clustering Tendency**.

To assist us in clustering tendency, we will use two R Packages.

```{r warning=FALSE, message=FALSE}
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("factoextra", "clustertend", prompt = FALSE)
```

## Get Data

To demo cluster tendency, we need data that illustrates clear clustering tendency (from the iris dataset) and one that is radomized.  Lets get this data now.

```{r getData}
df <- iris[,-5]
head(df)

random_df <- apply(df, 2, function(x){runif(length(x), min(x), (max(x)))})#this makes a matrix
#Recall the 2 in apply specifies columns
random_df <- as.data.frame(random_df)

#Normalize
df <- scale(df)
random_df <- scale(random_df)
```

## Visualize the Data

Since our data contains more than 2 variables, we need to reduce the dimensionality to do a scatter plot.  Use PCA to do this.  We will then leverage factoextra to plot.

```{r}
fviz_pca_ind(prcomp(df), title="PCA-Iris Data", habillage = iris$Species, palette = "jco", 
             geom="point", ggtheme=theme_classic())
# habillage:  color the individuals among a categorical variable (give the number of the categorical 
# supplementary variable or its name)

fviz_pca_ind(prcomp(random_df), title="PCA-Random Data", geom="point", ggtheme=theme_classic())
```

It is clear that the iris data has well-defined clusters where the random data has no such indication.  Therefore, we conclude we have data we can use to demo cluster tendency.

Lets see what the output is for performing k-means and hierarchial clustering on the random data.  For visualization, I will continue to use factoextra.

Here is a k-means visualization of the random data:

```{r}
kmeans1 <- kmeans(random_df, 3)
fviz_cluster(list(data = random_df, cluster=kmeans1$cluster), ellipse.type = "norm", geom="point", stand=FALSE,
             palette="jco", ggtheme=theme_classic())
```

Here is a Hierarchial visualization of the random data:

```{r warning=FALSE, message=FALSE}
fviz_dend(hclust(dist(random_df)), k=3, k_colors = "jco", as.ggplot=TRUE, show_labels=FALSE)
```

As illustrated above, even when there are no clusters in the data, the algorithms will still do its job and try to find some.  This can be misleading.  This is why cluster tendency is a required step for any clustere analysis.

## Cluster Tendency

There are two primary methods to assess clustering tendency:

- Hopkins Statistic
- Visual Assessment of Cluster Tendency (VAT)

### Hopkins Statistic

The Hopkins Statistic assess clustering tendency by measuring the probability that a given data set is generated by a uniform data dsitribution. It tests the spatial randomness of the data.

The Hopkins Static calculates a value, H.  A value where H = 0.5 suggests the data is uniformly distributed.  The null and alternative hypotheses are defined as:

- **Null Hypothesis** - The data is uniformly distributed suggesting there are no meaningful clusters
- **Alternative Hypothesis** - The data is not uniformly distribute suggests that meaningful clsuters exist in the data.  

If H is close to 0, then reject the null hypothesis and conclude the data is significantly clusterable.

Calculate H for the iris data:
```{r}
clustertend::hopkins(df, n=nrow(df)-1)
```

Calculate H for the random data:
```{r}
clustertend::hopkins(random_df, n=nrow(random_df)-1)
```

It is shown that the iris data is highly clusterable (H = ~ 0.17 which is significantly below the 0.50 threshold).  The random data is not clusterable since the H vlaue is close to the threshold.

### Visual Assessment of Cluster Tendency (VAT)

Using VAT, we compute the dissimilarity matrix between observations using the function *dist*.  Next the function *fviz_dist()* is used to display the matrix.

```{r}
fviz_dist(dist(df), show_labels = FALSE) + labs(title="Iris Data")
fviz_dist(dist(random_df), show_labels = FALSE) + labs(title="Random Data")
```

<span style="color:red">Red</span> - high similarity | <span style="color:blue">Blue</span> - low similarity.  The color level is proportional to the value of the dissimilarity between observations.

The matrix confirms there is cluster structure in iris but not in the random data.
