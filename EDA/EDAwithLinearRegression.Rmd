---
title: "EDA with Linear Regression"
output:
  rmdformats::readthedown:
    highlight: pygments
---

<style type="text/css">
p{ /* Normal  */
   font-size: 12px;
}
body{ /* Normal  */
   font-size: 12px;
}
td {  /* Table  */
   font-size: 10px;
}
h1 { /* Header 1 */
 font-size: 26px;
 color: #4294ce;
}
h2 { /* Header 2 */
 font-size: 22px;
}
h3 { /* Header 3 */
 font-size: 18px;
}
code.r{ /* Code block */
  font-size: 10px;
}
pre { /* Code block */
  font-size: 10px
}
#table-of-contents h2 {
background-color: #4294ce;
}
#table-of-contents{
background: #688FAD;
}
#nav-top span.glyphicon{
color: #4294ce;
}
#postamble{
background: #4294ce;
border-top: ;
}
</style>
---

```{r loadLibs1, warning=FALSE, message=FALSE, echo=FALSE}
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("plyr", "dplyr", "xda", "ggplot2", "readr", "gridExtra", "funModeling", 
     "corrplot", "caret", prompt = FALSE)
```

# Introduction

Linear Regression algorithm is not just for predicting the future. It is actually super useful for gaining practically useful insights about the relationships among the variables in data. Thanks to its simplicity, we can even explain the insights from the model to other people in a human language, not necessarily in a mathematical or scientific language.

How do we use Linear Regression to find insights about the relationship among the variables in a typical exploratory data analysis?

# Data

Queried US Natality data from Google’s hosted public data repository at BigQuery. It is a data set about baby births in US.  

```{r loadData, message=FALSE, results='hide'}
library(readr)
births <- read.csv("C:\\Users\\czwea\\Documents\\GitHub\\LargeDataFiles\\births.csv")
births <-select(births, -1)
df_status(births)
```

```{r}
births$mother_race <- mapvalues(births$mother_race, from = c(1, 2, 3, 4, 5, 6, 7), to = c("White", "Black", "American_Indian", "Chinese", "Japanese", "Hawaiian", "Filipino"))

births <- mutate(births, mother_race = case_when(mother_race == "68" ~ "Other_Asian",
                                                 mother_race =="18" ~ "Other_Asian",
                                                 mother_race =="78" ~ "Other_Asian",
                                                 mother_race =="48" ~ "Other_Asian",
                                                 mother_race =="28" ~ "Other_Asian",
                                                 mother_race =="38" ~ "Other_Asian",
                                                 mother_race =="58" ~ "Other_Asian",
                                                 TRUE ~ as.character(mother_race)))

births <- births %>% filter(!father_race == "99")

births$father_race <- mapvalues(births$father_race, from = c(1, 2, 3, 4, 5, 6, 7), to = c("White", "Black", "American_Indian", "Chinese", "Japanese", "Hawaiian", "Filipino"))

births <- mutate(births, father_race = case_when(father_race == "68" ~ "Other_Asian",
                                                 father_race =="18" ~ "Other_Asian",
                                                 father_race =="78" ~ "Other_Asian",
                                                 father_race =="48" ~ "Other_Asian",
                                                 father_race =="28" ~ "Other_Asian",
                                                 father_race =="38" ~ "Other_Asian",
                                                 father_race =="58" ~ "Other_Asian",
                                                 TRUE ~ as.character(father_race)))
```

## Explore Data

```{r}
freq(births, input = c("mother_race", "cigarette_use", "alcohol_use", "is_male"))
```

```{r message=FALSE}
freq(births, input = c("plurality"))
```

```{r eval=FALSE}
funModeling::plot_num(births)
```

```{r}
range(births$father_age)
#table(births$father_age)
nrow(filter(births, father_age ==99))
```

```{r}
births$father_age <- na_if(births$father_age, 99)
nrow(filter(births, father_age == 99))
```

Explore the data by using charts to see if there are any trends that would help understand the relationship between ``gestation_weeks`` variable and the other variables.

```{r}
sum(is.na(births$gestation_weeks))
sum(is.na(births$father_age))
```


```{r}
tmpBirths <- births %>% filter(!is.na(father_age)) %>% sample_n(5000)
ggplot(tmpBirths, aes(x = father_age, y = gestation_weeks)) + geom_point(shape = 1, alpha = 0.1, color = "blue") + 
     geom_smooth(method=lm)
```

We can’t really see an obvious correlation between the two variables. I’ve enabled a trend line (Linear Regression) inside the chart, and the line looks almost flat.

#Analysis 

## father_age & father_race

```{r message=FALSE, warning=FALSE}
#sum(is.na(tmpBirths$father_race))
#tmpBirths <- tmpBirths %>% filter(!is.na(father_race))

ggplot(tmpBirths, aes(x = father_age, y = gestation_weeks)) + geom_point(shape = 1, alpha = 0.1, color = "blue") + 
     geom_smooth(method=lm) + facet_wrap(~father_race)
```
Still not obvious, but some of the trend lines are showing upward or downward trends more than before. For example, the trend line for Japanese (38) seems to be showing a highly positive correlation, which means that as the father’s age gets older the `gestation_weeks` becomes longer. However, there seems to be not much data there, so I’m not sure how much we can count on this trend line at this point.

## By mother_age

Here is the similar scatter chart we have already seen above. This time, I’m assigning `mother_age` to X-Axis.

```{r}
#sum(is.na(tmpBirths$mother_age))

ggplot(tmpBirths, aes(x = mother_age, y = gestation_weeks)) + geom_point(shape = 1, alpha = 0.1, color = "blue") + 
     geom_smooth(method=lm) + facet_wrap(~mother_race)
```

It looks that there is a slight downward trend for a few races which means that as the mother’s age gets older the `gestation_weeks` becomes shorter.

## By `father_race`

How about the relationship between `father_race` and `gestation_weeks`. Here, I’m using a Bar chart to show the average `gestation_weeks` periods by each `father_race`. The red dotted line shows the overall average.

```{r}
fatherRace <- births %>% group_by(father_race) %>% summarise(meanGestation = mean(gestation_weeks))
ggplot(fatherRace, aes(as.factor(father_race), meanGestation)) + 
     geom_bar(stat="identity", color = "navy", fill = "navy", alpha = .3) +
     labs(x="`father_race`", y = "Average Gestation") +
     geom_hline(yintercept = 38.69, linetype = "dashed", color = "red")
```

It’s hard to see the difference, so I’m zooming in to the area around the overall average age, which is 38.69.

```{r}
ggplot(fatherRace, aes(as.factor(father_race), meanGestation)) + 
     geom_bar(stat="identity", color = "navy", fill = "navy", alpha = .3) +
     labs(x="`father_race`", y = "Average Gestation") +
     coord_cartesian(ylim = c(38, 39.2)) + 
     geom_hline(yintercept = 38.69, linetype = "dashed", color = "red")
```

Here, we can see some differences. Some races tend to have the `gestation_weeks` longer than the average. On the other hand, others tend to have the `gestation_weeks` shorter. However, we need to be reminded that the differences we are observing here are very small. They are all within less than a week.

## By `mother_race`

Let’s look at `nother_race` against `gestation_weeks` the same way we did for `father_race` above.

```{r}
motherRace <- births %>% group_by(mother_race) %>% summarise(meanGestation = mean(gestation_weeks))
ggplot(motherRace, aes(as.factor(mother_race), meanGestation)) + 
     geom_bar(stat="identity", color = "navy", fill = "navy", alpha = .3) +
     labs(x="`mother_race`", y = "Average Gestation") +
     coord_cartesian(ylim = c(38, 39.2)) + 
     geom_hline(yintercept = 38.69, linetype = "dashed", color = "red")
```

Simliar to `father_race`, some `mother_race` tend to be higher and lower than average. Blacks seem to be significantly lower.

## By Plurality

How about the relationship between `gestation_weeks` and `plurality`? Are twin or triplet (or even more!) babies are born earlier than single babies?

Here, I’m using a Bar chart assigning `plurality` to X-Axis and `gestation_weeks` to Y-Axis, and also showing a reference line (Red dotted line) to show the overall average of `gestation_weeks`.

```{r}
gestation <- births %>% group_by(as.factor(plurality)) %>% 
     summarise(meanGestation = mean(gestation_weeks))
names(gestation) <-  c("Plurality", "Average_Gestation")

ggplot(gestation, aes(Plurality, Average_Gestation)) + 
     geom_bar(stat="identity", color = "navy", fill = "navy", alpha = .3) +
     labs(x="Number of Babies Birthed", y = "Average Gestation") +
     geom_hline(yintercept = mean(gestation$Average_Gestation), linetype = "dashed", color = "red")
```

This one is actually easier to spot the trend. As `plurality` increases the average of the `gestation_weeks` becomes shorter.

Instead of comparing the average of `gestation_weeks` by using Bar chart, we can see how the values of `gestation_weeks` are distributed or spread for each `plurality` number by using Boxplot chart like below.

```{r}
ggplot(births, aes(as.factor(plurality), gestation_weeks)) + 
     geom_boxplot(color = "navy", fill = "navy", alpha = .3, na.rm = TRUE) +
     labs(x="Number of Babies Birthed", y = "Average Gestation")
```

Y-Axis shows `gestation_weeks` and X-Axis shows `plurality`. Each box represents the range between 25 percentile and 75 percentile of `gestation+weeks` and the center line inside each box indicates the median value of `gestation_weeks`.

This Boxplot chart helps us to compare the distribution of `gestation_weeks` among `plurality` categories. We can see a trend that `gestation_weeks` values tend to go down as the number of `plurality` goes up, though there are some overlaps between them. This leads us to think that there seems to be a negative correlation between `gestation_weeks` and `plurality`.

I’ve got a few hypotheses based on the observations we have made so far.

- `father_age` and `mother_age` don’t seem to be making much difference for `gestation_weeks`, though they might be making a relatively bigger difference within some of the `father_race` or M`mother_race`.
- `father_race` and `mother_race` seem to be making differences, though the difference is very small, it’s less than a week, in terms of the average `gestation_weeks`.
- `plurality` seems to be making a big difference for `gestation_weeks`.

Now, let’s evaluate these hypotheses by building Linear Regression models.

## Linear Regression Model Evaluation

Let’s start evaluating the above hypotheses one by one by building Linear Regression models.

*Correlation is different from Causation.*

But there is one thing to note before moving further. By performing the regression analysis with Linear Regression algorithm we can understand the relationships between the variables better. And we might find that some of the variables might be able to explain a large portion of the changes in `gestation_weeks`.

But the relationships between the variables we are talking about here are **correlation**, which means that the changes in one variable can be observed at a same pace as the changes occur in another variable. Therefore, I might say something like “the changes in `father_age` would influence on the changes in `gestation_weeks`”, but this doesn’t necessarily mean that the changes in `father_age` are causing '`gestation_weeks`'gestation_weeks` shorter or longer.

But, even if we don’t know if `father_age` is really causing the changes in `gestation_weeks`, just knowing that there is a correlation between them helps us predict or estimate how much `gestation_weeks` would change when we observe a certain amount of changes in `father_age`.

## gestation_weeks by father_age

```{r}
model_lm1 = lm(gestation_weeks ~ father_age, data = births)
summary(model_lm1)
```

**P-value** in this context is a probability of getting a similar set of changes even when there is no relationship between `father_age` and `gestation_weeks`. And if the probability is small enough (the very commonly known threshold is 5% but it can be higher or lower depending on the nature of the analysis.), then we would think that there has to be some degree of `father_age`’ influence on `gestation_weeks`.

Here, the P-value is a very small number. This means that if we assume that `father_age` doesn’t have any influence on the changes in `gestation_weeks`, the probability of getting a similar set of the changes in `gestation_weeks` is also very small. Therefore, we can practically conclude that this `father_age` has something to do with the changes in `gestation_weeks`.

The **coefficient estimate** here can be interpreted as, how much of the change in `gestation_weeks` can be explained (or influenced) by one value increase of `father_age`. The value here shows -0.009, which means that as the `father_age` becomes one year older the `gestation_weeks` would become 0.009 weeks shorter.

### Evaluate Quality of Prediction Model

To understand the quality of the model, there are many useful metrics, but here I want to introduce two practically useful ones.

**R Squared** measures how much of the variability of `gestation_weeks` this model can explain. The values vary between 0 and 1 in most cases and 1 is the highest, which means that the model can explain 100% of the variability of a target variable, in this case, that is `gestation_weeks`.  The R Squared is 0.0007 means it doesn’t really explain the variability of `gestation_weeks`.

So, what is the variability anyway?

Use the same Scatter chart presented earlier to explain.  It has been exploded to better serve as an explanatory example.

```{r}
ggplot(tmpBirths, aes(x = father_age, y = gestation_weeks)) + 
     geom_point(shape = 1, alpha = 0.1, color = "blue") + geom_smooth(method=lm, se= FALSE) +
     geom_hline(yintercept = 38.69, linetype = "dashed", color = "red") +
     coord_cartesian(ylim = c(35, 42.5)) 
```

Here, I’m showing `father_age` at X-Axis and `gestation_weeks` at Y-Axis, and each dot represents each baby. Also, I’m showing a Trend Line (Linear Regression Model) as Blue Line to show the trend of the relationship between `father_age` and `gestation_weeks` as a straight line, and a Reference Line as Red Line to show the Average of `gestation_weeks`.

Now, as you recall, the R-Squared for this model is 0.0007.

Well, this 0.0007 actually is a byproduct of the difference between the Blue and the Red lines. Compared to the average reference line (Red), the trend line created by Linear Regression model (Blue) covers a bit more of the `gestation_weeks` spread. But, it still stays within a range of 38 and 39 of `gestation_weeks` and is not explaining the whole range of `gestation_weeks` (mainly from 25 to 45) very well.

**Is this model useless with such low R Squared?**

This doesn’t mean that the coefficient estimate of `father_age` is not reliable. Whether we can conclude if there is any meaningful linear relationship between `father_age` and `gestation_weeks` is entirely up to the P-value, which is another metric under the model summary.

Since we have only one predictor variable, this model level P-value happens to be the same value of P-value for `father_race` variable. But when we start adding more variables, this P-value of the model will be different from the P-value for the coefficient of each variable.

Nonetheless, P-value here is quite a small number so we can practically conclude that this prediction model built with `father_age` can explain the changes in `gestation_weeks`, even if the amount of the change it can explain is very tiny.

Another useful metric is **Root Mean Square Error (RMSE)**. This shows the average difference between the actual values and the values this model would predict — Predicted Values. Here, it is showing as about 2.4.

This means that if we used this model to predict the `gestation_weeks` it would make an error (difference) of 2.4 weeks on average.

**Summary**

- We can conclude that `father_age` does have an impact on `gestation_weeks` and we know that this is not by a chance because the P-Value is quite small. And one year increase of `father_age` would tend to make `gestation_weeks` 0.009 weeks shorter.
- The prediction model built only with `father_age` can explain only 0.05% of the variability of `gestation_weeks`. And, if we use it to predict the `gestation_weeks` you would need to account for about 2.4 weeks errors (difference from the actual values).

> Given that one year increase in `father_age` would make `gestation_weeks` only 0.009 weeks shorter, 2.4 weeks error looks pretty big.

## mother_age

We have looked at the influence of `father_age` on `gestation_weeks`, but most of you must be thinking that actually `mother_age` should be the one having more influence on `gestation_weeks` than `father_age`. 

I have switched from `father_age` to `mother_age` for the predictor variable and re-built the linear regression model. 

```{r}
model_lm2 = lm(gestation_weeks ~ mother_age, data = births)
summary(model_lm2)
```

This time, the coefficient estimate of `mother_age` is -0.01, which means that one year increase of `mother_age` would make `gestation_weeks` 0.01 weeks shorter. It was -0.009 with `father_age`, so it seems to me that `mother_age` has more influencing power.

The R Squared is 0.001 and this is better than `father_age`.

The P-value is still nearly 0. This means, not only `mother_age` has something to do with the change in `gestation_weeks`.

Intuitively, we would think that older fathers tend to have babies with older mothers, and younger fathers tend to have babies with younger mothers.  ``father_age`` and ``mother_age`` are probably positively correlated. And if that is the case, when we say ``father_age`` influences on ``gestation_weeks`` as we investigated above, in reality, that might be because ``father_age`` and ``mother_age`` happen to be correlated and ``mother_age`` could be actually the one that is impacting on ``gestation_weeks``.

# Is `mother_age` Influencing or `father_age`?

So far, we know that the increases in `father_age` would make `gestation_weeks` shorter. And, the increases in `mother_age` would also make `gestation_weeks` shorter.

But wait for a second. . . aren't younger mothers tend to have babies with younger fathers? And the same goes for older mothers, aren’t older mothers tend to have babies with older fathers? Aren’t `father_age` and `mother_age` in this data positively correlated?

Take a look at this Scatter chart that has `father_age` at X-Axis and `mother_age` at Y-Axis below.

```{r}
tmpBirths <- births %>% filter(!is.na(father_age), mother_age) %>% sample_n(5000)
ggplot(tmpBirths, aes(x = father_age, y = mother_age)) + 
     geom_point(shape = 1, alpha = 0.1, color = "blue") + geom_smooth(method=lm)
```

`father_age` and `mother_age` are positively correlated, which means that when `father_age` increases `mother_age` also increases.

By using Correlation analysis we can see that the correlation between them is 0.75, which means they are moderately positively correlated.

> Below, note different values when calulating the Person coefficient (0.75) and the pairwise corrrelation (0.67)

Missing values and NAs imact the pairwise correlation calculations.  When calculating correlation between 2 variables with clean data, 0.75 is returned; otherwise 0.67 is returned.  Ideally, the values would be the same but data is dirty!

**Correlation = 0.75**

```{r message=FALSE}
set.seed(12345)
#Pearson
cor(tmpBirths$mother_age, tmpBirths$father_age, use = "complete.obs")
```
`cor.test` provides the same information and more:

```{r message=FALSE}
cor.test(tmpBirths$mother_age, tmpBirths$father_age)#gets the p value
```
```{r message=FALSE}
tmpBirths_1 <- tmpBirths %>% select(mother_age, father_age) %>% tidyr::drop_na()
correlation_table(tmpBirths_1, "mother_age")
```

This code is interesting - it performs a pairwise correlation but must manage the missing data to return the same data as the methods above!

```{r message=FALSE}
library("PerformanceAnalytics")
chart.Correlation(select_if(tmpBirths, is.numeric), histogram = TRUE, pch = 19)
```

**Pairwise Correlation: Correlation = 0.67** 

```{r message=FALSE}
#Pairwise Coefficients
cor(select_if(tmpBirths, is.numeric), use = "complete.obs")
```
```{r message=FALSE}
myCOR <- cor(select_if(tmpBirths, is.numeric), use = "complete.obs")
corrplot.mixed(myCOR, number.cex = .8, tl.pos = "lt")
```


**Is `father_age` alone really influencing the changes?**

Assume:  An older father had a baby recently, and the baby came out earlier than the average (Shorter `gestation_weeks`).  Now, is this because the father is older?  Or, maybe his wife or partner happened to be also older and could that be actually the direct influence of having the baby earlier than the average?

So instead of `father_age` directly influencing `gestation_weeks`, maybe `father_age` is influencing `mother_age` to be younger or older, and then `mother_age` ends up directly influencing `gestation_weeks` to be shorter or longer.

To answer this question we can bring `father_age` and `mother_age` together as the predictor variables and re-build the regression model.

When we have multiple predictor variables we can interpret each coefficient of the variables as *one point increase in a given predictor variable would increase or decrease a certain amount in the target variable when values in other variables stay the same*.

It means that we can say if one year increase in `father_age` would increase or decrease some amount of `gestation_weeks` when `mother_age` stays the same. We can separate the potential influence of `mother_age` from the influence of `father_age`. This goes the other way around, too. We can evaluate the `mother_age`’s influence alone without a potential influence of `father_age` by having these two variables as predictors.

## `father_age` and `mother_age`

Rebuild the Linear Regression model.

```{r}
set.seed(1)
model_lm3 <- lm(gestation_weeks ~ mother_age + father_age, data = tmpBirths)
summary(model_lm3)
```

The P-value for `father_age` variable has increased a lot and came to a point where we are not so sure if we should be confident that `father_age` is really influencing on `gestation_weeks` when `mother_age` stay the same.

Let’s take a look at `mother_age` variable. Its P-value is much smaller compared to the one for `father_age`.

So the probability of getting the similar changes in `gestation_weeks` without any influence of `mother_age` when `father_age` stays the same is very low. So if anything, `mother_age` can be more reliable to explain the changes in `gestation_weeks` than `father_age`. And this means that we can interpret that `mother_age` is probably the one that is more of a direct influence on `gestation_weeks`. And the reason `father_age` looked influencing on `gestation_weeks` in the previous analysis could be because it happened to be correlated well with `mother_age`.

The coefficient estimate of `mother_age` is -0.02, which means that one year increase in `mother_age` would make `gestation_weeks` 0.02 weeks shorter when `father_age` stays the same.

## Is `father_age` a Fake Variable?

Now that we are finding that `mother_age` is most likely the one influencing the changes in `gestation_weeks` more than `father_age`, does this mean that `father_age` doesn’t have any influence at all on `gestation_weeks`?

To answer this question, we can take a look at the model summary:

```{r}
summary(model_lm3)
```

R Squared is higher than the model with only `mother_age` (was Multiple R-squared:  0.001423). The value went up almost double. And this difference can be considered as a range of the variability of `gestation_weeks` that `mother_age` alone can’t explain but adding `father_age` can.

R Squared almost always goes up when you add more variables regardless whether they can really explain the changes in the target variable or not. To address this problem, we have a variation of R Squared called Adjusted R Squared, which penalizes for adding more variables, therefore, it can go down when you add variables that don’t contribute in explaining the variability of the target variable.

So, if `father_age` was not helpful at all then Adjusted R Squared could have decreased or stayed the same. The Adjusted R Squared has increased  (was Adjusted R-squared:  0.001422)! So we can interpret that adding `father_age` has some contributions in explaining the variability of `gestation_weeks`.

#Categorical Predictors

We have focused on investigating `father_age` and `mother_age`, but these variables happen to be both numerical data type. And, it’s actually pretty intuitive to understand the coefficient estimates of numerical variables when we say *one point increase in a given predictor variable would increase or decrease a certain amount of values in the target variable*. For example, we can say something like *One year increase in `mother_age` can make `gestation_weeks` 0.5 weeks shorter.*

But, how about categorical variables like `mother_race` whose values can be White, Black, Japanese, etc. What does it mean when we say “one point increase in `father_race` would increase or decrease `gestation_weeks` for some amounts?”

Before getting into the details, let’s first build a regression model by assigning only `mother_race` as the predictor variable.

```{r}
model_lm4 <- lm(gestation_weeks ~ factor(mother_race), data = births)
summary(model_lm4)
```

We assign one variable — `mother_race` — but what we are seeing at X-Axis here is a bunch of variables. When you look closer you would notice that each variable seems to be representing each unique value of `mother_race` variable.

Linear Regression function ‘lm’ in R automatically transforms a categorical variable into something called ‘dummy’ variables. It will create a column for each categorical value (e.g. Japanese) and have a value of 0 or 1 based on whether a given row matches a given column (e.g. Japanese or not).

Each of the dummy columns represents each `mother_race` type, except for one `mother_race` type, in this case, that is American Indian, which happens to be the base level of `mother_race` column. **How has the base level been decided**?

If a predictor variable is `character data type, then R’s lm function automatically picks the first unique value in an alphabetical order and set it to the base level. And, ‘American Indian` happens to be the first value.  This concept of the base level becomes very important when we try to understand the coefficients for the categorical variables.

We can see `mother_race — Black` P-value is very small. And we can interpret it as, *One value increase of `mother_race — Black` would decrease about 0.36 weeks in `gestation_weeks` when all the other variables stay the same.*  The last part is technically saying that all the other dummy variables are zero.

What we are seeing from the coefficient chart above tells us that `mother_race` being Black would influence `gestation_weeks` to be 0.36 weeks shorter compared to `mother_race` being `American Indian`.

## Changing Base Level

We know how to interpret the coefficient for the categorical variables. But, when we say that `mother_race — Black` would make the `gestation_weeks` 0.36 weeks shorter compared to American Indian, it is not necessarily straightforward to understand for many unless you happen to be an `American Indian` or be familiar with American Indian sociology. The problem is that unless we are very familiar with the base level value (e.g. `American Indian`) we don’t know what to do with such insights.

It would be easier to interpret the coefficient if we can change the base level to be something we are more familiar with. For example, if you happen to be a `Japanese`, then you might want to compare Black Mother to Japanese Mother. Or, you might want to compare any Race to the majority race like `White` in the data set.


A package called `forcats` from Hadley Wickham — a father of the modern R — provides a set of convenient functions to work with Factor data type. Use `fct_relevel` function to set the base level quickly.

```{r}
births$mother_race <- forcats::fct_relevel(births$mother_race, "White")
```

Now that `mother_race` is Factor type and its base level is `White`, we can re-build the model with the same configuration. Note that you need to make sure you have removed the filter that kept only American Indian and Black if you have created the filter before.

Once the model is built, we can see all the `mother_race`s are presented at X-Axis of the chart, except for `White` which is the base level now.

```{r}
model_lm4_2 <- lm(gestation_weeks ~ factor(mother_race), data = births)
summary(model_lm4_2)
```

And here again, `mother_race` — Black is the most statistically significant. And we can interpret it as, Black Mother tends to have `gestation_weeks` 0.47 weeks shorter compared to White Mother.

## Categorical Variables Can Be Correlated Too!

###Father and Mother Race

Just as we have investigated the relationship of `father_Age` and `mother_age` earlier, we would intuitively think that `father_race` and `mother_race` are also *correlated*, meaning that White mothers tend to have babies with White fathers, Black mothers tend to have babies with Black fathers, and so on.

Here is a chart that shows the ratio of the mothers in each race married to each `father_race` type.

```{r}
ggplot(tmpBirths, aes(x = mother_race, fill = father_race)) + geom_bar(position = "fill")
```

A majority of the mothers in most races had their babies with the fathers of their race.

The important point is `mother_race` and `father_race` seem to be correlated. If we know a given baby’s `mother_race`, then we can predict what her/his `father_race` is with a good accuracy rate for some of the races.

So again, we don’t know if `mother_race` is really the one influencing the changes in `gestation_weeks`. Maybe `mother_race` being Black means most likely `father_race` is also Black and that might be directly making the `gestation_weeks` shorter.

To find this out, again, we can turn to the same Linear Regression algorithm.

We can include both `mother_race` and `father_race` as the predictors and re-build the model. This will allow us to see the true impact of each `mother_race` type.

```{r}
model_lm5 <- lm(gestation_weeks ~ mother_race + father_race, data = births)
summary(model_lm5)
```

Now we can see that if `mother_race` is Black the `gestation_weeks` would become 0.39 weeks shorter compared to when `mother_race` is White assuming that `father_race` stays the same.

Also, we can notice that the coefficient of `father_race` being Black is not statistically significant.

We can practically conclude that it is `Mother Race` that would more directly influence the changes in `Gestation Week` than `Father Race`.

We have learned Black Mother tends to have about 0.36 weeks shorter `gestation_weeks` period compared to White Mother if we assume there is no influence of `father_race`. And `mother_race` seems to influence the changes in `gestation_weeks` more directly than `father_race`, though `father_race` also has some contributions to the changes.

Based on the findings so far, we know that `mother_age` and `mother_race` tend to have a reasonable amount of influence on the changes in `gestation_weeks`.

Now, as you might be wondering, the obvious question is, which of `mother_age` and `mother_race` is more directly influencing the changes in `gestation_weeks`? Maybe some of the `mother_race` tend to be older than the others. If that’s the case `mother_race` might be carrying over the influence of `mother_age`.

# Numerical and Categorical Predictors

So far, we have found that there are statistically significant relationships between `Mother Race` and `Gestation Week` and between `Mother Age` and `Gestation Week`.  Now, take a look at the relationship between `Mother Race` and `Mother Age` by using Boxplot chart.

```{r}
ggplot(sample_n(births, 100000), aes(as.factor(mother_race), mother_age)) + 
     geom_boxplot(color = "navy", fill = "navy", alpha = .3, na.rm = TRUE) +
     labs(x="Mother Race", y = "Mother Age")
```

We can see that some of the Mother Races like Chinese, Japanese, and Filipino tend to have babies at older ages than other races like White, Black, American Indian, and Hawaiian.

There seems to be some degree of correlation between `Mother Race` and `Mother Age`. Now, when we say “one value increase in `Mother Age` would tend to decrease `Gestation Week` for some degrees”, we are not sure if `Mother Age` alone is the one influencing `Gestation Week`. `Mother Age` might be carrying the influence of `Mother Race`. And this goes the other way around. When we say “Black mothers tend to have shorter gestation week compared to White mothers”, `Mother Race` alone might not be the one influencing `Gestation Week`, instead, it might be carrying the influence of `Mother Age`.

You know by now what to do to clarify this. Add them together as the predictor variables and re-build the Linear Regression model.

```{r}
model_lm4 = lm(gestation_weeks ~ mother_age+mother_race, data = births)
summary(model_lm4)
```

We can see that both `Mother Age` and `Mother Race—Black` are the two statistically significant variables and both of them have the negative impacts on `Gestation Week` meaning that one year increase in `Mother Age` would make `Gestation Week` shorter for some degrees while Mother being Black also would make `Gestation Week` shorter.

How much does Gestation Week get shorter?  If Mother Age stays the same, then Black Mothers tend to have 0.34 weeks shorter Gestation Week.

And, if Mother Race stays the same, then one year increase in `Mother Age` would make Gestation Week 0.017 weeks shorter.

Which of the two variables have more influences on the changes in Gestation Week?

- One year increase in `Mother Age` tends to have 0.017 weeks shorter `Gestation Week`.
- `Mother Race being Black` tends to have 0.34 weeks shorter `Gestation Week` compared to White.

So, does that mean Mother being Black can make a much bigger impact than Mother Age?

Not so quick. . . we are dealing with two completely different units here. One value change in `Mother Age` we are talking about is just one year and that is not much of the difference, intuitively speaking. But one value change in `Mother Race` means whether a given mother is Black or White, Japanese or White, etc., and that is a much bigger difference compared to the one year age difference.

So, when we look at the coefficients for these variables, we are not sure which of the two variables has a bigger influence on `Gestation Week` than the other.

This is when *T Ratio* comes in handy.

T Ratio is the metric that indicates how far from the values that can happen with the null hypothesis. It is calculated by dividing the coefficient by the standard error. With a large enough sample, its probability distribution called T-Distribution (similar to Normal Distribution).

Take a look at the T-Ratio values for `Mother Race` and `Mother Age`.  The t-ratio is the estimate divided by the standard error. With a large enough sample, t-ratios greater than 1.96 (in absolute value) suggest that your coefficient is statistically significantly different from 0 at the 95% confidence level. A threshold of 1.645 is used for 90% confidence.

```{r tRatio}
str(summary(model_lm4))

summary(model_lm4)$coefficients[,1]#Estimate
summary(model_lm4)$coefficients[,2]#Std.Error

myTratio <- summary(model_lm4)$coefficients[,1] / summary(model_lm4)$coefficients[,2]
```

Because `mother_age` is much farther out from -1.96 than `mother_raceBlack`, it is fair to conclude `mother_age` is more significant.

So far, we understand which variables are more significant in term of the power of influence on `Gestation Week` **when only one variable’s value changes**. But we still don’t know which variables would influence Gestation Week more than the others when their values change together.

Remember that the way we interpret the coefficients of Linear Regression model is:

> One value change in a given predictor variable can explain X number of change in the target variable when all the other predictor variables stay the same.

So, the model is actually not saying that `Mother Race` is more influential than `Mother Age`. Rather, the significance of the changes in `Gestation Week` that are influenced by `Mother Age` alone is bigger than by `Mother Race` alone.

If you want to know which variables are more influential on the changes in `Gestation Week` (or which variables are more useful to predict `Gestation Week` ), then there are other techniques we can use.  For example, we can use Random Forest algorithm, which can tell us which predictor variables have more influences on the target variable. This is called *Variable Importance*.

```{r}
myTreeData <- births %>% select(-c(5, 10, 12))
myTreeData <- myTreeData %>% tidyr::drop_na()
myTree <- train(gestation_weeks~., data = sample_n(myTreeData, 5000), method="rf", 
                trControl = trainControl(method="cv", number =2), importance=T)
varImp(myTree)
plot(varImp(myTree), top = 20)
```

Anoterh method to identify imporatnat variables is using `caret` `Recursive Feature Elminination` function.  (It is a version of backwards elimination.)  See https://topepo.github.io/caret/recursive-feature-elimination.html

```{r}
# Must convert character to factor
myTreeData2 <- myTreeData %>% mutate_if(is.character, as.factor)
# define the control using a random forest selection function
rfe_control <- rfeControl(functions=rfFuncs, method="cv", number=2)
# run the RFE algorithm
rfe_results <- rfe(myTreeData2[1:5000,1:12], myTreeData2[1:5000,13], sizes=c(1:12), rfeControl=rfe_control)
# summarize the results
```
```{r}
print(rfe_results)
# list the chosen features
predictors(rfe_results)
# plot the results
plot(rfe_results, type=c("g", "o"))
```

It appears a model with 5 variables might offer a good solution!

While `caret` offers nearly all the functions you will ever *need*, there are helpful functions that are not currently supported.  One of these is **Partial Dependency Plots** which tells us how the changes in each variable would influence the target variable values. 

The plot below shows how `Gestation Week` would change when `Mother Age` changes.  `Gestation Week` (Y-Axis) increases as `Mother Age` (X-Axis) increases up to age 25. And `Gestation Week` decreases as `Mother Age` increases up to age 47 when `Gestation Week` starts increasing again.  (The inflection values change based on the number of data points evalauted in the randomForest model which is limited by RAM.)

Given that a majority of the data resides between age 25 and 35 those upward trends might not be statistically significant.
```{r}
library(randomForest)
myRandomForest <- randomForest(gestation_weeks~., myTreeData2[1:5000,])
partialPlot(myRandomForest, myTreeData2[1:5000,], mother_age)
```

The relationship is not linear between the variables. As the name suggests, Linear Regression is not good at capturing non-linear relationships.

# Part 5

As we start building the models we would wonder which variables should be included as the predictors and which ones shouldn’t.
This is actually a big topic itself, but in this post, I am going to talk about what metrics to check when adding or removing the variables and how they change the values.

Lets add all the variables and build a Linear Regression model first.

```{r}
model_lm5 = lm(gestation_weeks ~ ., data = myTreeData2)#Note lm does not like NAs - eason for using myTreeData2
summary(model_lm5)
```

```{r}
#arm - https://druedin.com/2015/02/28/how-to-create-coefficient-plots-in-r-the-easy-way/
#alternative using ggplot: https://gist.github.com/dsparks/4332698
library(arm)
model_lm5$coefficients[1:5]
coefplot(model_lm5, vertical=F, col.pts = "blue", cex.var = 0.6)
```

R Squared is now 0.3073, this means that this model can explain 30% of the variability of Gestation Week. It’s not great, but it is the best so far in this series!

RMSE (Root Mean Square Error) is 2.03 so the average difference between the predicted values and the actual values is about 2 weeks.

All in all, the quality of the model is still not that great from the prediction point of view, though it has made a notable progress. But, as you have already known by now, what we are trying to do here is not about prediction, instead, we are trying to gain insights into the relationship between the variables.

Therefore, the most important metric here is actually `P-Value`. This model shows 0 as you can see above. This means that under the assumption that these predictor variables have nothing to do with the changes in `Gestation Week` — Null Hypothesis — the probability of observing the changes in `Gestation Week` by chance is almost 0 (P-Value), hence we can conclude that this model with all these variables have something to do with the changes in `Gestation Week` in a statistically significant way.

## How to Select Variables

For example, we have `Month` variable, but does the baby’s birth month really matter for `Gestation Week` to be shorter or longer? The P-Value for `Month` variable is 0.47.

Now, we have included all the variables as predictors, but do we really need them all?

### R Squared

As you recall, R Squared is useful for measuring how much of the variability of the target variable. In this example, 30% of Gestation Week’s variability can be explained by this model built with all the predictors presented here. If you are not sure what I’m talking about, take a look at the Part 1 episode where I introduced R Squared.

Now, the thing about R Squared is that it always increases when you add more predictor variables regardless of whether the added variables are meaningful in terms of explaining the changes in the target variable or not. So even when adding variables that are not meaningful we are not sure if the model quality is really getting improved or not.
And this can cause something called Overfitting, which happens when you build a prediction model that performs great with the training data but not with the real world data.
Adjusted R Squared and AIC
So, we want the measures that can tell us if we are adding or removing meaningful predictors. This is when the metrics like Adjusted R Squared and AIC become helpful.
Unlike R Squared, Adjusted R Squared doesn’t get better by just adding any variables. It penalizes for adding more variables so that it increases only when meaningful variables are added. If an added variable is not meaningful Adjusted R Squared would stay the same or decrease.
AIC (Akaike Information Criterion) is often used as a helpful measure to build models with fewer predictors while maintaining the prediction quality. The smaller the AIC value is the better the model is.
Now, this model without Month shows Adjusted R Squared as 0.29782851888, which is a tiny bit bigger than 0.2978245359 from the last model with Month. And AIC without Month is about 366,547, and it’s slightly smaller than 366,548 from the last model with Month.
So we can remove Month variable from the predictors not just because its P-Value is large, but also it doesn’t help explain the changes in Gestation Week.
How about Removing State?
When you look at the coefficient table, you would notice that State related variables (e.g. State — Ohio, State — Illinois, etc.) have higher P Values.


The idea of Gestation Week being different depending on which U.S. states you are born sounds kind of interesting. But does that really matters?
Let’s try removing State variable from the predictors and re-build the model.
Here is the model quality under Summary.


This model without State shows Adjusted R Squared as 0.29237787951, which is a bit smaller than the last one (0.29782851888). And AIC is about 367,211, which is a bit bigger than the last one (366,547). So, although some of the US States don’t have statistically significant influences on Gestation Week, thanks to other states like New York, Pennsylvania, Virginia, adding State variable to the model helps to explain the changes in Gestation Week better.
I’m highlighting the ‘dummy’ variables of US States that are statistically significant and can influence on Gestation Week.


In Exploratory, a categorical variable gets automatically transformed to have only the first 12 most frequent unique values and the rest become ‘Others’ by default
So it will become something like the chart below.


Now, State variable is categorical (character data type), which means that each ‘dummy’ variable needs to be interpreted in a comparison to the base level. If you’re not clear, take a look at Part 3 of this series where I explained how to interpret the coefficients for the categorical variables.
A Beginner’s Guide to EDA with Linear Regression — Part 3

How to Interpret Coefficients of Categorical Variables
blog.exploratory.io

Base Level for State
Now, what is the base level for this model?
We can find that out under Summary tab.


It is Arizona, and this is because it happens to be the first unique value in an alphabetical order. So each state is compared to Arizona in terms of how it is influencing Gestation Week.
Note that from Exploratory v4.3 the most frequent category becomes the base level. For example, with this data California would be the base level since it has the most entries.
We can use Bar chart to visualize the difference between Arizona and other States for their average Gestation Week like below.


The red dotted line shows Arizona’s average Gestation Week. As we can see California, Michigan, New York, Pennsylvania, Virginia shows longer Gestation Week compared to Arizona.
But by building Linear Regression with all the other predictors, we can conclude that only New York, Pennsylvania, and Virginia are the only states that are statistically significant when all the other variables stay the same.


Of course, you can change the base level to some other states that you are more familiar with as we have done in Part 3 if you like.
A Beginner’s Guide to EDA with Linear Regression — Part 3

How to Interpret Coefficients of Categorical Variables
blog.exploratory.io

Conclusion
We have learned that Month doesn’t add a value to the model in terms of explaining the changes in Gestation Week while State contributes increasing the explainability of the model for the changes in Gestation Week. When we were deciding which variables to be included or excluded in the model, we not only looked at P-Value for each variable but also evaluated some of the model metrics like Adjusted R Squared and AIC to see if they are valuable to be added in the model.
Now, when you look at the coefficient in the chart above you would notice that Weight Pound variable has the highest coefficient value (the most left hand side). But isn’t that kind of obvious that Weight Pound and Gestation Week are related? You will get a lighter weight baby if she/he is born earlier and you will get a heavier baby if she/he is born later.
In the next episode, I’m going to talk about how to deal with the variables that are correlated to one another. Such correlation can happen between the target variable and the predictor variable. Also, it can happen among the predictor variables. This latter case is specifically called as ‘Multi Collinearity’

 https://www.r-bloggers.com/analysis-of-covariance-–-extending-simple-linear-regression/