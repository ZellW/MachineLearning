---
title: 'Auto Val Unbalanced Data Experimentation'
output:
  rmdformats::readthedown:
    highlight: pygments
    code_folding: hide
---
<style type="text/css">
p{ /* Normal  */
   font-size: 14px;
   line-height: 18px;}
body{ /* Normal  */
   font-size: 14px;}
td {  /* Table  */
   font-size: 12px;}
h1 { /* Header 1 */
 font-size: 26px;
 color: #4294ce;}
h2 { /* Header 2 */
 font-size: 22px;}
h3 { /* Header 3 */
 font-size: 18px;}
code.r{ /* Code block */
  font-size: 12px;}
pre { /* Code block */
  font-size: 12px}
#table-of-contents h2 {
  background-color: #4294ce;}
#table-of-contents{
  background: #688FAD;}
#nav-top span.glyphicon{
  color: #4294ce;}
#postamble{
  background: #4294ce;
  border-top: ;}
</style>


```{r eval=FALSE, echo=FALSE, warning=FALSE, message=FALSE}
# The following two commands remove any previously installed H2O packages for R.
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }

# Next, we download packages that H2O depends on.
pkgs <- c("RCurl","jsonlite")
for (pkg in pkgs) {
if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
}

# Now we download, install and initialize the H2O package for R.
install.packages("h2o", type="source", repos="http://h2o-release.s3.amazonaws.com/h2o/latest_stable_r")

# Finally, let's load H2O and start up an H2O cluster
library(h2o)
h2o.init()
```

```{r}
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("plyr", "dplyr", "tidyr", "ggplot2", "caret", "ROSE", "DMwR", "readr", "h2o", "ROCR",  prompt = TRUE)
```

```{r setWD, echo=FALSE}
setwd("~/R/WIP") #change as needed
```

#Introduction

Inital modeling of the highly inbalanved has not produced acceptable results.  (For example, not predictins for `Skip` are output.)  This doceumtn explores traditiona methids of imporiving model performance through  over- and  under-sampling.

# Get Data

> Will use the smaller dataset to save time during experimentaton.

```{r getData, message=FALSE}
sample1 <- sample_n(read_csv("..//data/AutoValuation/sample1.csv"), 500000)

#glimpse(sample1)

sample1 <- sample1 %>% select(-c(active:skip), -person_id)

sample1 <- sample1 %>% rename(mode = event_type, choice = decision)

sample1$choice[sample1$choice == 1] <- "TRUE"
sample1$choice[sample1$choice == 0] <- "FALSE"

sample1 <- sample1 %>% filter(choice == "TRUE")
sample1$choice <- NULL

sample1$mode[sample1$mode == 1] <- "Active"
sample1$mode[sample1$mode == 2] <- "Repo"
sample1$mode[sample1$mode == 3] <- "Skip"

sample1 <- sample1 %>% mutate_at(vars("mode"), funs(factor(.)))

sample1 <- data.frame(sample1)

glimpse(sample1)

tmpDF <- sample1 %>% count(mode) %>% mutate(prop = prop.table(n))

#sample2 <- ImpSampClassif(mode ~ ., sample1, C.perc = list(Active = 1, Repo = 8, Skip = 20)
```

##Unbalanced data

In this context, unbalanced data refers to classification problems where we have unequal instances for different classes. Having unbalanced data is actually very common in general, but it is especially prevalent when working with disease data where we usually have more healthy control samples than disease cases. Even more extreme unbalance is seen with fraud detection, where e.g. most credit card uses are okay and only very few will be fraudulent. 

Below, Repo is 10.5% and Skip is 1.5% of the toal record count.

```{r}
tmpDF
```

# Balance Data for Modeling

The basic theoretical concepts behind over and under-sampling are very simple:

- With under-sampling, randomly select a subset of samples from the class with more instances to match the number of samples coming from each class. The main disadvantage of under-sampling is relevant information is potentially lost from the left-out samples.
- With oversampling, randomly duplicate samples from the class with fewer instances or generate additional instances based on the data available, to match the number of samples in each class.  The risk of overfitting becomes a potential issue becuase it is more likely to get the same samples in the training and in the test data, i.e. the test data is no longer independent from training data. This would lead to an overestimation of the model performance and generalizability.

Some Rules of Thumb

- Consider testing under-sampling when you have an a lot data (tens- or hundreds of thousands of instances or more)
- Consider testing over-sampling when you don't have a lot of data (tens of thousands of records or less)
- Consider testing random and non-random (e.g. stratified) sampling schemes.
- Consider testing different resampled ratios (e.g. you don't have to target a 1:1 ratio in a binary classification problem, try other ratios)

Use cross-validation and perform over- or under-sampling on each fold independently to get an estimate of model performance.

```{r dataSplit}
set.seed(42)
index <- createDataPartition(sample1$mode, p = 0.7, list = FALSE)
train_data <- sample1[index, ]
test_data  <- sample1[-index, ]
```
```{r eval=FALSE}
model_rf <- caret::train(mode ~ ., data = train_data, method = "ranger", 
          preProcess = c("scale", "center"), 
          trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE, classProbs=TRUE))

final_rf <- data.frame(actual = test_data$mode, predict(model_rf, newdata = test_data, type = "prob"))
final_rf <- final_rf %>% mutate(Predicted = case_when(
  Skip > 0.50 ~ "Skip", Repo > 0.50 ~ "Repo", TRUE ~ "Active") %>% as.factor())

saveRDS(model_rf, "baseRangerModel.RDS")
saveRDS(final_rf, "final_rf.RDS")
```
```{r echo=FALSE}
model_rf <- readRDS("baseRangerModel.RDS")
final_rf <- readRDS("final_rf.RDS")

cm_original <- confusionMatrix(final_rf$Predicted, test_data$mode)
cm_original
```

#### Under-sampling

`caret` makes it easy to incorporate over- and under-sampling techniques with cross-validation resampling. Simply add the sampling option to `trainControl` and choose down for under- (also called down-) sampling. The rest stays the same.

```{r underModel, eval=FALSE}
model_rf_under <- caret::train(mode ~ ., data = train_data, method = "ranger", 
          preProcess = c("scale", "center"), 
          trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE, sampling = "down", classProbs=TRUE))
saveRDS(model_rf_under, "RangerModel_under.RDS")

final_under <- data.frame(actual = test_data$mode, predict(model_rf_under, newdata = test_data, type = "prob"))
final_under <- final_under %>% mutate(Predicted = case_when(
  Skip > 0.50 ~ "Skip", Repo > 0.50 ~ "Repo", TRUE ~ "Active") %>% as.factor())
saveRDS(final_under, "final_under.RDS")
#final_under$predict <- ifelse(final_under$benign > 0.5, "benign", "malignant")
```
```{r}
model_rf_under <- readRDS("baseRangerModel.RDS")
final_under <- readRDS("final_under.RDS")
cm_under <- confusionMatrix(final_under$Predicted, test_data$mode)
cm_under
```

#### Oversampling

For over- (also called up-) sampling we simply specify sampling = "up".

```{r overModel, warning=FALSE, eval=FALSE}
model_rf_over <- caret::train(mode ~ ., data = train_data, method = "ranger", 
          preProcess = c("scale", "center"), 
          trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE, sampling = "up", classProbs=TRUE))

final_over <- data.frame(actual = test_data$mode, predict(model_rf_over, newdata = test_data, type = "prob"))
final_over <- final_over %>% mutate(Predicted = case_when(
  Skip > 0.50 ~ "Skip", Repo > 0.50 ~ "Repo", TRUE ~ "Active") %>% as.factor())

saveRDS(model_rf_over, "RangerModel_over.RDS")
saveRDS(final_over, "final_over.RDS")
#final_over$predict <- ifelse(final_over$benign > 0.5, "benign", "malignant")
```

```{r}
model_rf_over <- readRDS("RangerModel_over.RDS")
final_over <- readRDS("final_over.RDS")
cm_over <- confusionMatrix(final_over$Predicted, test_data$mode)
cm_over
```

#### ROSE

Besides over- and under-sampling, there are hybrid methods that combine under-sampling with the generation of additional data. Two of the most popular are `ROSE` and `SMOTE`.

ROSE: ROSE (Random Over-Sampling Examples) provides functions to deal with binary classification problems in the presence of imbalanced classes. Artificial balanced samples are generated according to a smoothed bootstrap approach and allow for aiding both the phases of estimation and accuracy evaluation of a binary classifier in the presence of a rare class. Functions that implement more traditional remedies for the class imbalance and different metrics to evaluate accuracy are also provided. These are estimated by holdout, bootstrap, or cross-validation methods.

You implement them the same way as before, this time choosing sampling = "rose".

> ROSE can be effective but is limited to binary classification problems.

```{r roseModel, warning=FALSE, eval=FALSE}
model_rf_rose <- caret::train(mode ~ ., data = train_data, method = "ranger", 
          preProcess = c("scale", "center"), 
          trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE, sampling = "rose", classProbs=TRUE))
saveRDS(model_rf_rose, "RangerModel_rose.RDS")

final_rose <- data.frame(actual = test_data$classes, predict(model_rf_rose, 
                     newdata = test_data, type = "prob"))

#final_rose$predict <- ifelse(final_rose$benign > 0.5, "benign", "malignant")

cm_rose <- confusionMatrix(final_rose$predict, test_data$classes)
cm_rose
```

#### SMOTE

Choose sampling = "smote" in the trainControl settings.

SMOTE:  A combination of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. 

Advantages 

- Mitigates the problem of overfitting caused by random oversampling as synthetic examples are generated rather than replication of instances
- No loss of useful information

Disadvantages 

- While generating synthetic examples SMOTE does not take into consideration neighboring examples from other classes. This can result in increase in overlapping of classes and can introduce additional noise
- SMOTE is not very effective for high dimensional data

```{r smoteModel, eval=FALSE}
model_rf_smote <- caret::train(mode ~ ., data = train_data, method = "ranger", preProcess = c("scale", "center"), 
          trControl = trainControl(method = "cv", number = 5, verboseIter = TRUE, sampling = "smote", classProbs=TRUE))

final_smote <- data.frame(actual = test_data$mode, predict(model_rf_smote, 
                      newdata = test_data, type = "prob"))

final_smote <- final_smote %>% mutate(Predicted = case_when(
  Skip > 0.50 ~ "Skip", Repo > 0.50 ~ "Repo", TRUE ~ "Active") %>% as.factor())
#final_smote$predict <- ifelse(final_smote$benign > 0.5, "benign", "malignant")
saveRDS(model_rf_smote, "RangerModel_smote.RDS")
saveRDS(final_smote, "final_smote.RDS")
```
```{r}
model_rf_smote <- readRDS("RangerModel_smote.RDS")
final_smote <- readRDS("final_smote.RDS")
cm_smote <- confusionMatrix(final_smote$Predicted, test_data$mode)
cm_smote
```

### Compare Predictions

Compare the predictions of all these models:

```{r compareModels}
models <- list(original = model_rf, under = model_rf_under, over = model_rf_over, 
          smote = model_rf_smote)

resampling <- resamples(models)
bwplot(resampling)
```
```{r warning=FALSE}
comparison <- data.frame(model = names(models), Sensitivity = rep(NA, length(models)), 
               Specificity = rep(NA, length(models)),
               Precision = rep(NA, length(models)), Recall = rep(NA, length(models)), 
               F1 = rep(NA, length(models)))

for(name in names(models)){
  model <- get(paste0("cm_", name))
  myTMP_DF <- as.data.frame(model$byClass)#Need to make DF to avoid error: 
  #Evaluation error: $ operator is invalid for atomic vectors.
  
  comparison[comparison$model == name, ] <- filter(comparison, model == name) %>%
    dplyr::mutate(Sensitivity = myTMP_DF[1,1], Specificity = myTMP_DF[2,1],
           Precision = myTMP_DF[5,1], Recall = myTMP_DF[6,1],
           F1 = myTMP_DF[7,1])
}
comparison %>% gather(x, y, Sensitivity:F1) %>% ggplot(aes(x = x, y = y, color = model)) +
    geom_jitter(width = 0.2, alpha = 0.5, size = 3)
```

With this small dataset, we can already see how the different techniques can influence model performance. 

- Sensitivity (or recall) describes the proportion of benign cases that have been predicted correctly
- Specificity describes the proportion of malignant cases that have been predicted correctly
- Precision describes the true positives, i.e. the proportion of benign predictions that were actual from benign samples. 
- F1 is the weighted average of precision and sensitivity/ recall.

This shows a simple example of how to correct for unbalance in datasets for machine learning. For more advanced instructions and potential caveats with these techniques, check out the excellent caret documentation.

# Autoencoder

Train the unsupervised neural network model using deep learning autoencoders. With `h2o`, simply set autoencoder = TRUE.

Applying a technique called "bottleneck" training, where the hidden layer in the middle is very small. This means that the model will have to reduce the dimensionality of the input data (in this case, down to 2 nodes/dimensions).

The autoencoder model will learn the patterns of the input data irrespective of given class labels. Here, it will learn, which auto val modes  outcomes are similar and which transactions are outliers or anomalies (`Repo` and `Skip`). 

> Autoencoder models are sensitive to outliers which might throw off otherwise typical patterns

To make this analysis a bit easier, `Repo` and `Skip` will be combined into one variable to transform the analysis to a binary classification.

The interger 1 repsetns `Repo` and `Skip`.  Active is represented by 0.

```{r}
sample_nn <- sample1 %>% mutate(mode = ifelse(mode != "Active", 1, 0))
sample_nn$mode <- as.integer(sample_nn$mode) %>% as.factor()
unique(sample_nn$mode)
```

```{r h2o, warning=FALSE, message=FALSE}
h2o.init()
splits <- h2o.splitFrame(as.h2o(sample_nn), ratios = c(0.4, 0.4), seed = 42)

train_unsupervised  <- splits[[1]]
train_supervised  <- splits[[2]]
test <- splits[[3]]

response <- "mode"
features <- setdiff(colnames(train_unsupervised), response)
```

```{r}
model_nn <- h2o.deeplearning(x = features,
                             training_frame = train_unsupervised,
                             model_id = "model_nn",
                             autoencoder = TRUE,
                             reproducible = TRUE, #slow - turn off for real problems
                             ignore_const_cols = FALSE,
                             seed = 42,
                             hidden = c(10, 2, 10), 
                             epochs = 100,
                             activation = "Tanh")

#Convert to autoencoded representation
test_autoenc <- h2o.predict(model_nn, test)
```

```{r eval=FALSE}
h2o.saveModel(model_nn, force = TRUE)
model_nn <- h2o.loadModel("model_nn")
model_nn
```

## Dimensionality reduction with hidden layers

Because a bottleneck model wa sused with two nodes in the hidden layer in the middle, we can use this dimensionality reduction to explore our feature space (similar to what to we could do with a principal component analysis). We can extract this hidden feature with the `h2o.deepfeatures()` function and plot it to show the reduced representation of the input data.

```{r}
train_features <- h2o.deepfeatures(model_nn, train_unsupervised, layer = 2) %>%
  as.data.frame() %>% mutate(mode = as.vector(train_unsupervised[, 6]))
ggplot(train_features, aes(x = DF.L2.C1, y = DF.L2.C2, color = mode)) + geom_point(alpha = 0.1)
```

Here, we do not see a cluster of 0's that is distinct from non-`Active` instances, so dimensionality reduction with our autoencoder model alone is not sufficient to identify 1's in this dataset.

But we could use the reduced dimensionality representation of one of the hidden layers as features for model training. An example would be to use the 10 features from the first or third hidden layer:

```{r}
# let's take the third hidden layer
train_features <- h2o.deepfeatures(model_nn, train_unsupervised, layer = 3) %>% as.data.frame() %>%
  mutate(mode = as.factor(as.vector(train_unsupervised[, 6]))) %>% as.h2o()

features_dim <- setdiff(colnames(train_features), response)

model_nn_dim <- h2o.deeplearning(y = response,
                               x = features_dim,
                               training_frame = train_features,
                               reproducible = TRUE, #slow - turn off for real problems
                               balance_classes = TRUE,
                               ignore_const_cols = FALSE,
                               seed = 42,
                               hidden = c(10, 2, 10), 
                               epochs = 100,
                               activation = "Tanh")
```

For measuring model performance on test data, we need to convert the test data to the same reduced dimensions as the trainings data:

```{r}
test_dim <- h2o.deepfeatures(model_nn, test, layer = 3)

h2o.predict(model_nn_dim, test_dim) %>% as.data.frame() %>% mutate(actual = as.vector(test[, 6])) %>%
  group_by(actual, predict) %>% summarise(n = n()) %>% mutate(freq = n / sum(n))
```

##Anomaly Detection

`h2o.anomaly()` can indicate which records are outliers or anomalies. Based on the autoencoder model that was trained before, the input data will be reconstructed and for each instance, the mean squared error (MSE) between actual value and reconstruction is calculated.

Also calculating the mean MSE for both class labels.

```{r}
anomaly <- h2o.anomaly(model_nn, test) %>% as.data.frame() %>% tibble::rownames_to_column() %>% mutate(mode = as.vector(test[, 6]))

mean_mse <- anomaly %>% group_by(mode) %>% summarise(mean = mean(Reconstruction.MSE))

ggplot(anomaly, aes(x = as.numeric(rowname), y = Reconstruction.MSE, color = as.factor(mode))) +
  geom_point(alpha = 0.3) +
  geom_hline(data = mean_mse, aes(yintercept = mean, color = as.factor(mode))) +
  scale_color_brewer(palette = "Set1") + labs(x = "instance number", color = "mode")
```

There is no perfect classification into `Active` and `Inactive` cases.

Now identify outlier instances by applying an MSE threshold for what we consider outliers. We could e.g. say that we consider every instance with an MSE > ?? (chosen according to the plot above) to be an anomaly/outlier.

```{r}
anomaly <- anomaly %>% mutate(outlier = ifelse(Reconstruction.MSE > 0.02, "outlier", "no_outlier"))

anomaly %>% group_by(mode, outlier) %>% summarise(n = n()) %>% mutate(freq = n / sum(n)) 
```

Uncertain if outlier detection is sufficient to correctly classify `Inactive` cases (at least not with this dataset).

##Pre-trained supervised model

Try using the autoencoder model as a pre-training input for a supervised model again using a neural network. This model will now use the weights from the autoencoder for model fitting.

```{r}
model_nn_2 <- h2o.deeplearning(y = response,
                               x = features,
                               training_frame = train_supervised,
                               pretrained_autoencoder  = "model_nn",
                               reproducible = TRUE, #slow - turn off for real problems
                               balance_classes = TRUE,
                               ignore_const_cols = FALSE,
                               seed = 42,
                               hidden = c(10, 2, 10), 
                               epochs = 100,
                               activation = "Tanh")
#h2o.saveModel(model_nn_2, path="model_nn_2", force = TRUE)
```


```{r}
pred <- as.data.frame(h2o.predict(object = model_nn_2, newdata = test)) %>% mutate(actual = as.vector(test[, 6]))
pred %>% group_by(actual, predict) %>% summarise(n = n()) %>% mutate(freq = n / sum(n)) 
```

```{r}
pred %>% ggplot(aes(x = actual, fill = predict)) + geom_bar() + theme_bw() +
    scale_fill_brewer(palette = "Set1") + facet_wrap( ~ actual, scales = "free", ncol = 2)
```

##Measuring model performance on highly unbalanced data

Because of the severe bias towards `Active` cases, we can not use performance measures like accuracy or area under the curve (AUC), as they would give overly optimistic results based on the high percentage of correct classifications of the majority class.

An alternative to AUC is to use the precision-recall curve or the sensitivity (recall)-specificity curve. To calculate and plot these metrics, use  `ROCR`. There are different ways to calculate the area under a curve (see the PRROC package for details) but I am going to use a simple function that calculates the area between every consecutive points-pair of x (i.e. x1 - x0, x2 - x1, etc.) under the corresponding values of y.

```{r, rocr}
library(ROCR)
# http://stackoverflow.com/questions/24563061/computing-integral-of-a-line-plot-in-r
line_integral <- function(x, y) {
  dx <- diff(x)
  end <- length(y)
  my <- (y[1:(end - 1)] + y[2:end]) / 2
  sum(dx * my)
} 

pred_obj <- prediction(pred$p1, pred$actual)
par(mfrow = c(1, 2))
par(mar = c(5.1,4.1,4.1,2.1))

# precision-recall curve
perf1 <- performance(pred_obj, measure = "prec", x.measure = "rec") 

x <- perf1@x.values[[1]]
y <- perf1@y.values[[1]]
y[1] <- 0

plot(perf1, main = paste("Area Under the\nPrecision-Recall Curve:\n", round(abs(line_integral(x,y)), digits = 3)))

# sensitivity-specificity curve
perf2 <- performance(pred_obj, measure = "sens", x.measure = "spec") 

x <- perf2@x.values[[1]]
y <- perf2@y.values[[1]]
y[1] <- 0

plot(perf2, main = paste("Area Under the\nSensitivity-Specificity Curve:\n", round(abs(line_integral(x,y)), digits = 3)))
```

Precision is the proportion of test cases predicted to be fraud that were indeed fraudulent (i.e. the true positive predictions), while recall or sensitivity is the proportion of fraud cases that were identified as fraud. And specificity is the proportion of non-fraud cases that are identified as non-fraud.

The precision-recall curve tells us the relationship between correct fraud predictions and the proportion of fraud cases that were detected (e.g. if all or most fraud cases were identified, we also have many non-fraud cases predicted as fraud and vice versa). The sensitivity-specificity curve thus tell us the relationship between correctly identified classes of both labels (e.g. if we have 100% correctly classified fraud cases, we will have no correctly classified non-fraud cases and vice versa).

We can also look at this a little bit differently, by manually going through different prediction thresholds and calculating how many cases were correctly classified in the two classes:

```{r}
thresholds <- seq(from = 0, to = 1, by = 0.1)
pred_thresholds <- data.frame(actual = pred$actual)

for (threshold in thresholds) {
  
  prediction <- ifelse(pred$p1 > threshold, 1, 0)
  prediction_true <- ifelse(pred_thresholds$actual == prediction, TRUE, FALSE)
  pred_thresholds <- cbind(pred_thresholds, prediction_true)

}

colnames(pred_thresholds)[-1] <- thresholds
pred_thresholds %>%
  gather(x, y, 2:ncol(pred_thresholds)) %>%
  group_by(actual, x, y) %>%
  summarise(n = n()) %>%
  ggplot(aes(x = as.numeric(x), y = n, color = actual)) +
    geom_vline(xintercept = 0.6, alpha = 0.5) +
    geom_line() +
    geom_point(alpha = 0.5) +
    theme_bw() +
    facet_wrap(actual ~ y, scales = "free", ncol = 2) +
    labs(x = "prediction threshold",
         y = "number of instances")
```

This plot tells us that we can increase the number of correctly classified non-fraud cases without loosing correctly classified fraud cases when we increase the prediction threshold from the default 0.5 to 0.6:

```{r}
pred %>% mutate(predict = ifelse(pred$p1 > 0.6, 1, 0)) %>% group_by(actual, predict) %>% summarise(n = n()) %>% mutate(freq = n / sum(n)) 
```


### Example using class_imbalance

```{r balance_classes_example, eval=FALSE}
data(iris)  #Not required?
iris <- iris[1:120,] #Remove 60% of virginica
summary(iris$Species) #50/50/20

d <- as.h2o(iris)
splits = h2o.splitFrame(d,0.8,c("train","test"), seed=77)
train = splits[[1]]
test = splits[[2]]
summary(train$Species)  #41/41/14
summary(test$Species)  #9/9/6

m1 = h2o.randomForest(1:4, 5, train, model_id ="RF_defaults", seed=1)
h2o.confusionMatrix(m1)

m2 = h2o.deeplearning(1:4, 5, train, model_id ="RF_balanced", seed=1,
  balance_classes = TRUE)
h2o.confusionMatrix(m2)

m3 = h2o.randomForest(1:4, 5, train, model_id ="RF_balanced", seed=1,
  balance_classes = TRUE,
  class_sampling_factors = c(1, 1, 2.5)
  )
h2o.confusionMatrix(m3)
```



# Cliff WIP

```{r}
# https://www.reddit.com/r/statistics/comments/14fv87/help_needed_multinomial_logistic_regression_in_r/
library(glmnet)

model_GLMNET <- train(mode~., train_data, method='glmnet', tuneGrid=expand.grid(.alpha=0:1, .lambda=0:30/10)) 
plot(model_GLMNET)

saveRDS(model_GLMET, "GLMNET.RDS")
```

```{r}
model_GLMNET <- readRDS("GLMNET.RDS")
```


```{r}

sample1_scaled <- sample1 %>% mutate_if(is.numeric, scale)
model_multi <- nnet::multinom(mode~., data = train_data, SoftMax=TRUE, maxit=500)

library(caret)
mostImportantVariables <- varImp(model_multi)
mostImportantVariables$Variables <- row.names(mostImportantVariables)
mostImportantVariables <- mostImportantVariables[order(-mostImportantVariables$Overall),]
print(head(mostImportantVariables))

preds2 <- predict(model_multi, type = "class", newdata = test_data)

postResample(test_data$mode, preds2)


summary(model_multi)

predicted_scores <- predict(model_multi, test_data, "probs")

predicted_class <- predict(model_multi, test_data)
table(predicted_class, test_data$mode)

mean(as.character(predicted_class) != as.character(test_data$mode))#misclassification error

totalAccuracy <- c()
cv <- 20
cvDivider <- floor(nrow(sample1_scaled) / (cv+1))
 
for (cv in seq(1:cv)) {
  # assign chunk to data test
  dataTestIndex <- c((cv * cvDivider):(cv * cvDivider + cvDivider))
  dataTest <- sample1[dataTestIndex,]
  # everything else to train
  dataTrain <- sample1[-dataTestIndex,]
 
  model_multi <- nnet::multinom(mode~., data=dataTrain, maxit=1000, trace=T) 
 
  pred <- predict(model_multi, newdata=dataTest, type="class")
 
  #  classification error
  cv_ac <- postResample(dataTest$mode, pred)[[1]]
  print(paste('Current Accuracy:',cv_ac,'for CV:',cv))
  totalAccuracy <- c(totalAccuracy, cv_ac)
  mean(totalAccuracy)
}
```

# softmax https://shiring.github.io/machine_learning/2017/05/01/fraud


### Reduce Model Sizes

The models devloped using `ranger1` are large models that exhaust the RAM on my laptop.  Explore ways to *trim the fat* from these models.  A solution will be developed using the `model_rf`.  These results of this will then be applied to the other models.

How big is the first model? (It is BIG!) Hint, the over-sampling model is much bigger!)

```{r}
print(paste("Size before pruning:", format(object.size(model_rf), unit="Mb")))
#Alternative
#length(serialize(model_rf, NULL))
```

Break down this into its constituent parts:

```{r}
mySize <- ldply(names(model_rf), function(v) {
  v.size <- format(object.size(model_rf[[v]]), unit="Mb")
  data.frame(variable=v, size=v.size)
})
mySize[order(as.numeric(mySize$size), decreasing=TRUE),]
```
```{r}
mySize2 <- ldply(names(model_rf$finalModel), function(v) {
  v.size2 <- format(object.size(model_rf$finalModel[[v]]), unit="Mb")
  data.frame(variable=v, size=v.size2)
})
mySize[order(as.numeric(mySize2$size), decreasing=TRUE),]
```

# Reference

1. https://shiring.github.io/machine_learning/2017/04/02/unbalanced