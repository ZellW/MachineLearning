---
title: "Simple Tokenization with tidytext"
output:
  prettydoc::html_pretty:
    theme: Architect
    highlight: github
---

```{r message=FALSE, warning=FALSE, echo=FALSE}
setwd("~/R/Complete")
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("tidyverse", "tidytext", "itunesr", prompt = FALSE)
```

# Introduction

Tokenization is often teh first step in NLP.

n-grams, bi-grams, etc. - simple!

# Data

Use `itunesr` for downloading iOS App Reviews on which we’ll perform Simple Text Analysis (unigrams, bigrams, n-grams). `getReviews()` of `itunesr` helps extract reviews of Medium iOS App.

```{r}
medium <- getReviews("828256236","us",1)

head(medium)
```

There are two Text Columns of interest - Title and Review.

To make our n-grams analysis a bit more meaningful, extract only the positive reviews (5-star) to see what’s good people are writing about Medium iOS App. 

```{r}
table(medium$Rating)
```

5-star is the major component in the text reviews. Pick `Review` and specify only for `Rating == 5`.

```{r}
reviews <- data.frame(txt = medium$Review[medium$Rating==5],stringsAsFactors = FALSE)
reviews <- reviews %>% filter(Rating ==5)
```

# Tokens

Tokenization in NLP is the process of splitting a text corpus based on some splitting factor - It could be Word Tokens or Sentence Tokens or based on some advanced alogrithm to split a conversation.

```{r}
reviews %>% unnest_tokens(output = word, input = txt) %>% head()
```

`unnest_tokens()` is the function that helps in the tokenization process.

```{r}
reviews %>% unnest_tokens(output = word, input = txt) %>% count(word, sort = TRUE) 
```

# Stopword Removal

```{r}
reviews %>% unnest_tokens(output = word, input = txt) %>% anti_join(stop_words) %>% count(word, sort = TRUE) 
```

# unigram Visualization

```{r}
reviews %>% unnest_tokens(output = word, input = txt) %>% anti_join(stop_words) %>% count(word, sort = TRUE) %>% slice(1:10) %>% 
  ggplot() + geom_bar(aes(word, n), stat = "identity", fill = "#de5833") +
  theme_minimal() + labs(title = "Top unigrams of Medium iOS App Reviews",subtitle = "using Tidytext in R",
                         caption = "Data Source: itunesr - iTunes App Store")
```

# Bigrams & N-grams

Slightly modify the above codet by adding a new argument n=2 and token="ngrams" to the tokenization process to extract n-gram.

Doing this naively also has a catch - the stop-word removal process used above was using `anti_join` which would not be supported in this process since a bigram was developed (two-word combination separated by a space). Therefore, separate the word by space and then filter out the stop words in both word1 and word2 and then unite them back - which gives returns the bigram after stop-word removal.

```{r}
reviews %>% 
  unnest_tokens(word, txt, token = "ngrams", n = 2) %>% 
  separate(word, c("word1", "word2"), sep = " ") %>% 
  filter(!word1 %in% stop_words$word) %>% filter(!word2 %in% stop_words$word) %>% 
  unite(word,word1, word2, sep = " ") %>% count(word, sort = TRUE) %>% slice(1:10) %>% 
  ggplot() + geom_bar(aes(word, n), stat = "identity", fill = "#de5833") +
  theme_minimal() + coord_flip() +
  labs(title = "Top Bigrams of Medium iOS App Reviews", subtitle = "using Tidytext in R",
       caption = "Data Source: itunesr - iTunes App Store")
```
