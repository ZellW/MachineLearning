input_data <- test_data[1, ] %>%  select(-class)
# predict test case using model
pred <- predict(model_rf, input_data)
cat("----------------\nTest case predicted to be", as.character(pred), "\n----------------")
save.image("API2.RData")
# take first test case for prediction
input_data <- test_data[1, ] %>%  select(-class)
# predict test case using model
pred <- predict(model_rf, input_data)
cat("----------------\nTest case predicted to be", as.character(pred), "\n----------------")
var_names <- model_rf$finalModel$xNames
var_names
# show parameter definition for the first three features
for (i in 1:3) {
# if you wanted to see it for all features, use
#for (i in 1:length(var_names)) {
var <- var_names[i]
train_data_subs <- train_data[, which(colnames(train_data) == var)]
type <- class(train_data_subs)
if (type == "numeric") {
min <- min(train_data_subs)
max <- max(train_data_subs)}
cat("Variable:", var, "is of type:", type, "\n",
"Min value in training data =", min, "\n",
"Max value in training data =", max, "\n----------\n")}
library(rjson)
test_case_json <- toJSON(input_data)
cat(test_case_json)
test_case_json <- toJSON(input_data)
cat(test_case_json)
library(plumber)
r <- plumb("My_API.R")
r$run(port = 8000)
library(plumber)
r <- plumb("My_API.R")
#r$run(port = 8000)
r$run(host="127.0.0.1", port=8000, swagger=TRUE)
setwd("~/GitHub/R_Programming/API_Testing")
# options
options(echo=TRUE)
options(stringsAsFactors=FALSE)
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("tidyverse", "rpart",  "rpart.plot", "jsonlite", "plumber", prompt = FALSE)
packages("farff",  "missForest", "dummies", "caret", "lime",
"funModeling",  "json", prompt = FALSE)
data_file <- file.path("Chronic_Kidney_Disease.arff")
#load data with the farff package
data <- readARFF(data_file)
df_status(data)
data_imp <- missForest(data, verbose = FALSE)
data_imp_final <- data_imp$ximp
data_dummy <- dummy.data.frame(dplyr::select(data_imp_final, -class), sep = "_")
data <- cbind(dplyr::select(data_imp_final, class), scale(data_dummy, center = apply(data_dummy, 2, min), scale = apply(data_dummy, 2, max)))
glimpse(data)
# training and test set
set.seed(42)
index <- createDataPartition(data$class, p = 0.9, list = FALSE)
train_data <- data[index, ]
test_data  <- data[-index, ]
# modeling
model_rf <- caret::train(class ~ ., data = train_data, method = "rf", # random forest
trControl = trainControl(method = "repeatedcv", number = 10, repeats = 5, verboseIter = FALSE))
model_rf
load("API2.RData")
# take first test case for prediction
input_data <- test_data[1, ] %>%  select(-class)
# predict test case using model
pred <- predict(model_rf, input_data)
cat("----------------\nTest case predicted to be", as.character(pred), "\n----------------")
var_names <- model_rf$finalModel$xNames
var_names
# show parameter definition for the first three features
for (i in 1:3) {
# if you wanted to see it for all features, use
#for (i in 1:length(var_names)) {
var <- var_names[i]
train_data_subs <- train_data[, which(colnames(train_data) == var)]
type <- class(train_data_subs)
if (type == "numeric") {
min <- min(train_data_subs)
max <- max(train_data_subs)}
cat("Variable:", var, "is of type:", type, "\n",
"Min value in training data =", min, "\n",
"Max value in training data =", max, "\n----------\n")}
test_case_json <- toJSON(input_data)
cat(test_case_json)
library(plumber)
r <- plumb("My_API2.R")
r$run(port = 8000)
library(plumber)
r <- plumb("My_API2.R")
r$run(port = 8000)
setwd("~/GitHub/R_Programming/API_Testing")
# options
options(echo=TRUE)
options(stringsAsFactors=FALSE)
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("tidyverse", "rpart",  "rpart.plot", "jsonlite", "plumber", prompt = FALSE)
install.packages("caret")
install.packages("rpart")
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("tidyverse", "keras", prompt = TRUE)
setwd("~/GitHub/MachineLearning")
imdb <- dataset_imdb(num_words = 10000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb
class(train_data)
train_data[1]
class(train_labels)
vectorize_sequences <- function(sequences, dimension = 10000) {
# Create an all-zero matrix of shape (len(sequences), dimension)
results <- matrix(0, nrow = length(sequences), ncol = dimension)
for (i in 1:length(sequences))
# Sets specific indices of results[i] to 1s
results[i, sequences[[i]]] <- 1
results}
# Our vectorized training data
x_train <- vectorize_sequences(train_data)
# Our vectorized test data
x_test <- vectorize_sequences(test_data)
# Our vectorized labels
y_train <- as.numeric(train_labels)
y_test <- as.numeric(test_labels)
class(x_train)
x_train(1)
x_train[1]
x_train[2]
x_train[1:10]
x_train[1:100]
l2_model <- keras_model_sequential() %>%
layer_dense(units = 16, kernel_regularizer = regularizer_l2(0.001),
activation = "relu", input_shape = c(10000)) %>%
layer_dense(units = 16, kernel_regularizer = regularizer_l2(0.001),
activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
l2_model %>% compile(
optimizer = "rmsprop",
loss = "binary_crossentropy",
metrics = c("acc"))
l2_model_hist <- l2_model %>% fit(
x_train, y_train,
epochs = 20,
batch_size = 512,
validation_data = list(x_test, y_test))
plot_training_losses(losses = list(
original_model = original_hist$metrics$val_loss,
l2_model = l2_model_hist$metrics$val_loss))
plot_training_losses <- function(losses) {
loss_names <- names(losses)
losses <- as.data.frame(losses)
losses$epoch <- seq_len(nrow(losses))
losses %>% gather(model, loss, loss_names[[1]], loss_names[[2]]) %>%
ggplot(aes(x = epoch, y = loss, colour = model)) + geom_point()}
plot_training_losses(losses = list(
original_model = original_hist$metrics$val_loss,
l2_model = l2_model_hist$metrics$val_loss))
original_hist <- original_model %>% fit(x_train, y_train, epochs = 20, batch_size = 512, validation_data = list(x_test, y_test))
original_model <- keras_model_sequential() %>%
layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%
layer_dense(units = 16, activation = "relu") %>%
layer_dense(units = 1, activation = "sigmoid")
original_model %>% compile(optimizer = "rmsprop", loss = "binary_crossentropy", metrics = c("accuracy"))
original_hist <- original_model %>% fit(x_train, y_train, epochs = 20, batch_size = 512, validation_data = list(x_test, y_test))
devtool::remotes::install_github("rstudio/gt")
remotes::install_github("rstudio/gt")
library(gt)
problem <- c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1")
last <- c("sigmoid", "softmax", "sigmoid", "None", "sigmoid")
loss <- c("binary_crossentropy", "categorical_crossentropy", "binary_crossentropy", "MSE",  "MSE or binary_crossentropy")
mtTbl_Df <- data.frame(problem, last, loss)
myTbl_Df <- data.frame(problem, last, loss)
gt_tbl <- gt(data = myTbl_Df)
gt_tbl
gt_tbl %>% tab_header(title = "**Selecting Last Layer Activation and Loss**")
gt_tbl <-  gt_tbl[1,] %>% tab_header(title = "**Selecting Last Layer Activation and Loss**")
gt_tbl
gt_tbl[1,]
gt_tbl <-  gt_tbl[1,] %>% tab_header(title = md("**Selecting Last Layer Activation and Loss**"))
gt_tbl
gt_tbl <-
gt_tbl[1,] %>% tab_header(
title = md("**Selecting Last Layer Activation and Loss**")) %>%
cols_label(problem = "Problem", last = "Last Layer Activation", loss = "Loss Function")
gt_tbl
Problem <- c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1")
last <- c("sigmoid", "softmax", "sigmoid", "None", "sigmoid")
loss <- c("binary_crossentropy", "categorical_crossentropy", "binary_crossentropy", "MSE",  "MSE or binary_crossentropy")
myTbl_Df <- data.frame(problem, last, loss)
problem <- c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1")
last <- c("sigmoid", "softmax", "sigmoid", "None", "sigmoid")
loss <- c("binary_crossentropy", "categorical_crossentropy", "binary_crossentropy", "MSE",  "MSE or binary_crossentropy")
myTbl_Df <- data.frame(problem, last, loss)
gt_tbl <- gt(data = myTbl_Df)
gt_tbl <-
gt_tbl[1,] %>% tab_header(
title = md("**Selecting Last Layer Activation and Loss**")) %>%
cols_label(Problem = "Problem",
last = "Last Layer Activation",
loss = "Loss Function")
gt_tbl <- gt(data = myTbl_Df)
gt_tbl <-
gt_tbl[1,] %>% tab_header(
title = md("**Selecting Last Layer Activation and Loss**")) %>%
cols_label(problem = "Problem",
last = "Last Layer Activation",
loss = "Loss Function")
gt_tbl
tab_1 <-
countrypops %>%
dplyr::select(-contains("code")) %>%
dplyr::filter(country_name == "Mongolia") %>%
tail(5) %>%
gt() %>%
cols_label(
country_name = "Name",
year = "Year",
population = "Population"
)
tab_1
tab_2 <-
countrypops %>%
dplyr::select(-contains("code")) %>%
dplyr::filter(country_name == "Mongolia") %>%
tail(5) %>%
gt() %>%
cols_label(
country_name = md("**Name**"),
year = md("**Year**"),
population = md("**Population**"))
tab_2
tab_1 <-
countrypops %>%
dplyr::select(-contains("code")) %>%
dplyr::filter(country_name == "Mongolia") %>%
tail(5) %>%
gt() %>%
cols_label(
country_name = "Name",
year = "Year",
population = "Population"
)
tab_1
countrypops
install.packages("utf8")
countrypops
countrypops %>%
dplyr::select(-contains("code")) %>%
dplyr::filter
countrypops %>%
dplyr::select(-contains("code")) %>%
dplyr::filter(country_name == "Mongolia") %>%
tail(5)
gt_tbl <- gt(data = myTbl_Df) %>% cols_label(problem = "Problem",
last = "Last Layer Activation",
loss = "Loss Function")
gt_tbl
gt_tbl <-
gt_tbl[1,] %>% tab_header(
title = md("**Selecting Last Layer Activation and Loss**"))
gt_tbl
gt_tbl <- gt(data = myTbl_Df) %>% cols_label(problem = "Problem",
last = "Last Layer Activation",
loss = "Loss Function")
gt_tbl
gt_tbl <- gt(data = myTbl_Df) %>% cols_label(
problem = md("**Problem**"),
last = md("**Last Layer Activation**"),
loss = md("**Loss Function**"))
gt_tbl
gt_tbl <- gt(data = myTbl_Df) %>% cols_label(
problem = md("**Problem**"),
last = md("**Last Layer Activation**"),
loss = md("**Loss Function**")) %>%
tab_header(md("**Selecting Last Layer Activation and Loss**"))
gt_tbl
tibble(
problem <- c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1"),
last <- c("sigmoid", "softmax", "sigmoid", "None", "sigmoid"),
loss <- c("binary_crossentropy", "categorical_crossentropy", "binary_crossentropy",
"MSE",  "MSE or binary_crossentropy"))
library(dplyr)
gt_tbl <- tibble(
problem <- c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1"),
last <- c("sigmoid", "softmax", "sigmoid", "None", "sigmoid"),
loss <- c("binary_crossentropy", "categorical_crossentropy", "binary_crossentropy",
"MSE",  "MSE or binary_crossentropy")) %>%
gt(data = myTbl_Df) %>% cols_label(
problem = md("**Problem**"),
last = md("**Last Layer Activation**"),
loss = md("**Loss Function**")) %>%
tab_header(md("**Selecting Last Layer Activation and Loss**"))
gt_tbl <- tibble(
problem <- c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1"),
last <- c("sigmoid", "softmax", "sigmoid", "None", "sigmoid"),
loss <- c("binary_crossentropy", "categorical_crossentropy", "binary_crossentropy",
"MSE",  "MSE or binary_crossentropy")) %>%
gt() %>% cols_label(
problem = md("**Problem**"),
last = md("**Last Layer Activation**"),
loss = md("**Loss Function**")) %>%
tab_header(md("**Selecting Last Layer Activation and Loss**"))
tibble(
problem <- c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1"),
last <- c("sigmoid", "softmax", "sigmoid", "None", "sigmoid"),
loss <- c("binary_crossentropy", "categorical_crossentropy", "binary_crossentropy",
"MSE",  "MSE or binary_crossentropy"))
gt_tbl <- dplyr::tibble(
problem <- c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1"),
last <- c("sigmoid", "softmax", "sigmoid", "None", "sigmoid"),
loss <- c("binary_crossentropy", "categorical_crossentropy", "binary_crossentropy",
"MSE", "MSE or binary_crossentropy"))
dplyr::tibble(
problem <- c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1"),
last <- c("sigmoid", "softmax", "sigmoid", "None", "sigmoid"),
loss <- c("binary_crossentropy", "categorical_crossentropy", "binary_crossentropy",
"MSE", "MSE or binary_crossentropy"))
dplyr::tibble(
problem <- c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1")
dplyr::tibble(
tibble(
problem <- c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1")
gt_tbl <- dplyr::tibble(
dplyr::tibble(
problem <- c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1")
)
dplyr::tibble(
problem <- c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1"),
last <- c("sigmoid", "softmax", "sigmoid", "None", "sigmoid")
)
dplyr::tibble(
problem <- c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1"),
last <- c("sigmoid", "softmax", "sigmoid", "None", "sigmoid"),
loss <- c("binary_crossentropy", "categorical_crossentropy", "binary_crossentropy",
"MSE", "MSE or binary_crossentropy"))
dplyr::tibble(
problem <- c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1"),
last <- c("sigmoid", "softmax", "sigmoid", "None", "sigmoid"),
loss <- c("binary_crossentropy", "categorical_crossentropy", "binary_crossentropy",
"MSE", "MSE or binary_crossentropy")
)
dplyr::tibble(
pixels = px(seq(10, 35, 5)),
image = seq(10, 35, 5)
)
dplyr::tibble(
pixels = c("q", "w", "d"),
image = c("c", "m", "p")
)
dplyr::tibble(
problem <- c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1"),
last <- c("sigmoid", "softmax", "sigmoid", "None", "sigmoid")
)
dplyr::tibble(
problem = c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1"),
last = c("sigmoid", "softmax", "sigmoid", "None", "sigmoid"),
loss = c("binary_crossentropy", "categorical_crossentropy", "binary_crossentropy",
"MSE", "MSE or binary_crossentropy"))
#Note below when creating tibble must use  = not <-
gt_tbl <- dplyr::tibble(
problem = c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1"),
last = c("sigmoid", "softmax", "sigmoid", "None", "sigmoid"),
loss = c("binary_crossentropy", "categorical_crossentropy", "binary_crossentropy",
"MSE", "MSE or binary_crossentropy")) %>%
gt() %>% cols_label(
problem = md("**Problem**"),
last = md("**Last Layer Activation**"),
loss = md("**Loss Function**")) %>%
tab_header(md("**Selecting Last Layer Activation and Loss**"))
gt_tbl
tab_1 <-
countrypops %>%
dplyr::filter(country_name == "Mongolia") %>%
dplyr::select(-contains("code")) %>%
tail(10) %>%
gt() %>%
data_color(
columns = vars(population),
colors = scales::col_numeric(
palette = c(
"red", "orange", "green", "blue"),
domain = c(0.2E7, 0.4E7))
)
tab_1
gt_tbl %>% tab_style(style = tab_header(bkgd_color = "lightcyan"))
gt_tbl %>% tab_style(style = cells_styles(
bkgd_color = "lightcyan"),
locations = tab_header())
gt_tbl %>% tab_style(style = cells_styles(
bkgd_color = "lightcyan"),
locations = title)
gt_tbl %>% tab_style(style = cells_styles(
bkgd_color = "lightcyan"),
locations = tab_header)
gt_tbl %>% tab_style(style = cells_title (
bkgd_color = "lightcyan"),
locations = tab_header)
gt_tbl %>% tab_style(style = cells_stles (
bkgd_color = "lightcyan"),
locations = cells_header)
gt_tbl %>% tab_style(style = cells_stles (
bkgd_color = "lightcyan"),
locations = cells_title)
gt_tbl %>% tab_style(style = cells_styles (
bkgd_color = "lightcyan"),
locations = cells_title)
gt_tbl
gt_tbl %>% tab_options(
heading.background.color = "blue"
)
gt_tbl %>% tab_options(
heading.background.color = "teal"
)
gt_tbl %>% tab_options(
heading.background.color = "#605887"
)
gt_tbl %>% tab_options(
heading.background.color = "#3d5657"
)
#Note below when creating tibble must use  = not <-
gt_tbl <- dplyr::tibble(
problem = c("Binary classification", "Multiclass, single-label classification",
"Multiclass, multilabel classification", "Regression to arbitrary values",
"Regression to values between 0 and 1"),
last = c("sigmoid", "softmax", "sigmoid", "None", "sigmoid"),
loss = c("binary_crossentropy", "categorical_crossentropy", "binary_crossentropy",
"MSE", "MSE or binary_crossentropy")) %>%
gt() %>% cols_label(
problem = md("**Problem**"),
last = md("**Last Layer Activation**"),
loss = md("**Loss Function**")) %>%
tab_header(md("**Selecting Last Layer Activation and Loss**")) %>%
tab_options(heading.background.color = "#3d5657")
gt_tbl
install.packages("remedy")
remedy::remedy_opts$get('hotkeys')
rm(list = ls())
library(keras)
model <- keras_model_sequential() %>%
layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu", input_shape = c(28, 28, 1)) %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu") %>%
layer_max_pooling_2d(pool_size = c(2, 2)) %>%
layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu")
model
model <- model %>%
layer_flatten() %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 10, activation = "softmax")
model
mnist <- dataset_mnist() c(c(train_images, train_labels), c(test_images, test_labels)) %<-% mnist
mnist <- dataset_mnist()
c(c(train_images, train_labels), c(test_images, test_labels)) %<-% mnist
train_images <- array_reshape(train_images, c(60000, 28, 28, 1)) train_images <- train_images / 255
mnist <- dataset_mnist()
c(c(train_images, train_labels), c(test_images, test_labels)) %<-% mnist
train_images <- array_reshape(train_images, c(60000, 28, 28, 1))
train_images <- train_images / 255
test_images <- array_reshape(test_images, c(10000, 28, 28, 1))
test_images <- test_images / 255
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
model %>% compile(optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy"))
model %>% fit(train_images, train_labels, epochs = 5, batch_size=64)
results <- model %>% evaluate(test_images, test_labels)
results
getwd()
setwd("~/GitHub/MachineLearning")
getwd()
setwd("~/GitHub/MachineLearning")
getwd()
#Need to keep all these - get rid of mnist?
save.image(file = "./Deep_Learning_with_r/convnet1.RData")
