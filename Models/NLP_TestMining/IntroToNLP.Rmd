---
title: 'Getting started in NLP: Tokenization tutorial'
output:
  rmdformats::readthedown:
    highlight: pygments
---

<style type="text/css">
p{ /* Normal  */
   font-size: 12px;
}
body{ /* Normal  */
   font-size: 12px;
}
td {  /* Table  */
   font-size: 10px;
}
h1 { /* Header 1 */
 font-size: 26px;
 color: #4294ce;
}
h2 { /* Header 2 */
 font-size: 22px;
}
h3 { /* Header 3 */
 font-size: 18px;
}
code.r{ /* Code block */
  font-size: 10px;
}
pre { /* Code block */
  font-size: 10px
}
#table-of-contents h2 {
background-color: #4294ce;
}
#table-of-contents{
background: #688FAD;
}
#nav-top span.glyphicon{
color: #4294ce;
}
#postamble{
background: #4294ce;
border-top: ;
}
</style>


```{r echo=FALSE, warning=FALSE, message=FALSE}
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("tidyverse", "tidytext", "glue", "data.table", prompt = FALSE)
```

# Introduction

One common task in NLP (Natural Language Processing) is tokenization. `Tokens` are usually individual words (at least in languages like English) and `tokenization` is taking a text or set of text and breaking it up into its individual words. These tokens are then used as the input for other types of analysis or tasks, like parsing (automatically tagging the syntactic relationship between words).

# The Data

We will use a corpus of transcribed speech from bilingual children speaking in English. You can find more information on this dataset and download it here.

This dataset of kid's speech is really cool, but it's in a bit of a weird file format. These files were generated byÂ CLAN, a specialized program for transcribing children's speech. Under the hood, however, they're just text files with some additional formatting. With a little text processing we can just treat them like raw text files.
Let's do that, and find out if there's a relationship between how often different children use disfluencies (words like "um" or "uh") and how long they've been exposed to English.

```{r loadLibs, message=FALSE, warning=FALSE}
# load in libraries we'll need
library(tidyverse) #keepin' things tidy
library(tidytext) #package for tidy text analysis (Check out Julia Silge's fab book!)
library(glue) #for pasting strings
library(data.table) #for rbindlist, a faster version of rbind
```

```{r getData}
# now let's read in some data & put it in a tibble (a special type of tidy dataframe)
file_info <- read.csv("../../data/speech/guide_to_files.csv")
head(file_info)
```

Ok, that all looks good. Now, let's take the file names we have in that .csv and read one of them into R.

# Data Cleaning

```{r}
# glue - Expressions enclosed by braces will be evaluated as R code.
# stick together the path to the file & 1st file name from the information file
fileName <- glue("../../data/speech/", as.character(file_info$file_name[1]), sep = "")
# get rid of any sneaky trailing spaces
fileName <- trimws(fileName)
# read in the new file
fileText <- paste(readLines(fileName))
# and take a peek!
head(fileText)
# what's the structure?
str(fileText)
```

Each line is a separate element. We want the count of actual words. We're only interested in looking at the words that the child is using, not the experimenter. Looking at the docs, we can see that the child's speech is only on the lines that start with "*CHI: Child speaking". So we can use regular expressions to only look at lines that start with that exact string.

```{r}
# "grep" finds the elements in the vector that contain the exact string *CHI:.
# (You need to use the double slashes becuase want to match the character
# *, and usually that means "match any character"). We then select those indexes from
# the vector "fileText".
childsSpeech <- as_data_frame(fileText[grep("\\*CHI:",fileText)])
head(childsSpeech)
```

Now we have a tibble of sentences that the child said. That still doesn't get us closer to answering our question of how many many times this child said "um" (transcribed here as "&-um").

Let's start by making our data tidy. `Tidy data` has three qualities:

1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.

```{r}
# use the unnest_tokens function to get the words from the "value" column of "child
childsTokens <- childsSpeech %>% unnest_tokens(word, value)#tidytext
head(childsTokens)
```

Much better! Note that the `unnest_tokens` function has also done a lot of the work of preprocessing for us. Punctuation has been removed, and everything has been made lowercase. You don't always want to do this, but for this use case its very hands: we don't want "trees" and "Trees" to be counted as two different words.

# Text Analysis

Now, let's look at word frequencies:

```{r}
# look at just the head of the sorted word frequencies
childsTokens %>% count(word, sort = T) %>% head
```

#Text Visualizations

The most frequent word isn't actually something the child said: it's the annotation that the child is speaking, or "chi"! We're going to need to get rid of that. Let's do that by using "anti_join", from dplyr.

```{r}
# anti_join removes any rows that are in the both dataframes, so I make a data_frame
# of 1 row that contins "chi" in the "word" column.
sortedTokens <- childsSpeech %>% unnest_tokens(word, value) %>% anti_join(data_frame(word = "chi")) %>% count(word, sort = T)
head(sortedTokens)
```

That is what we wanted... but only for one file. We want to be able to compare across all the files. To do that, let's streamline our workflow a bit.

```{r}
# let's make a function that takes in a file and exactly replicates what we just did
fileToTokens <- function(filename){
     # read in data
     fileText <- paste(readLines(filename))
     # get child's speech
     childsSpeech <- as_data_frame(fileText[grep("\\*CHI:",fileText)])
     # tokens sorted by frequency 
     sortedTokens <- childsSpeech %>% unnest_tokens(word, value) %>% anti_join(data_frame(word = "chi")) %>% count(word, sort = T)
     # and return that to the user
     return(sortedTokens)
}
```

Now that we have our function, let's run it over a file to check that it's working.

```{r}
# we still have this fileName variable we assigned at the beginning of the tutorial
fileName
# so let's use that...
head(fileToTokens(fileName))
# and compare it to the data we analyzed step-by-step
head(sortedTokens)
```

Great, the output from our function is exactly the same as the output from the analysis we did step-by-step! Now let's do it over the entire set of files.

One thing we do need to do is point out which child said which words. To do that, we're going to add a coulmn to the output of this function every time we run it with the file that we're runnning it over.

```{r}
# let's write another function to clean up file names. (If we can avoid 
# writing/copy pasting the same code we probably should)
prepFileName <- function(name){
     # get the filename
     fileName <- glue("../../data/speech/", as.character(name), sep = "")
     # get rid of any sneaky trailing spaces
     fileName <- trimws(fileName)
     
     # can't forget to return our filename!
     return(fileName)
}
# make an empty dataset to store our results in
tokenFreqByChild <- NULL

# becuase this isn't a very big dataset, we should be ok using a for loop
# (these can be slow for really big datasets, though)
for(name in file_info$file_name){
     # get the name of a specific child
     child <- name
     
     # use our custom functions we just made!
     tokens <- prepFileName(child) %>% fileToTokens()
     # and add the name of the current child
     tokensCurrentChild <- cbind(tokens, child)
     
     # add the current child's data to the rest of it
     # I'm using rbindlist here becuase it's much more efficent (in terms of memory usage) than rbind
     tokenFreqByChild <- rbindlist(list(tokensCurrentChild,tokenFreqByChild))
}

# make sure our resulting dataframe looks reasonable
summary(tokenFreqByChild)
head(tokenFreqByChild)
```

Ok, now we've got the data for all the child in one dataframe. Let's do some visualization!
```{r}
# let's plot the how many words get used each number of times 
ggplot(tokenFreqByChild, aes(n)) + geom_histogram()
```

This visualiation tells us that most words are only used once, and that there are fewer words that are used more. This is a very robust pattern in human language (it's known as *Zipf's Law*), so it's no surprise we're seeing it here!

Now, back to our original question. Let's see if there's a relationship between the frequency of the term "um" and how long a child has been learning language.

```{r}
#first, let's look at only the rows in our dataframe where the word is "um"
ums <- tokenFreqByChild[tokenFreqByChild$word == "um",]
# now let's merge our ums dataframe with our information file
umsWithInfo <- merge(ums, file_info, by.y = "file_name", by.x = "child")
head(umsWithInfo)
```

That looks good. Now let's see if there's a relationship between the number of times a child said "um" and how many months of English exposure they'd had.

```{r}
# see if there's a significant correlation
cor.test(umsWithInfo$n, umsWithInfo$months_of_english)

# and check the plot
ggplot(umsWithInfo, aes(x = n, y = months_of_english)) + geom_point() + 
geom_smooth(method = "lm")
```

That's a resounding "no"; there is absolutely no relation between the number of months a child in this corpus had been exposed to English and the number of times they said "um" during data elicitation.

There are some things that could be done to make this analysis better:

- Look at relative frequency (out of all the words a child said, what proportion were "um") rather than just raw frequency
- Look at all disfluencies together ("uh", "um", "er", etc.) rather than just "um"
- Look at unintelligible speech ("xxx")


**Reference**: http://blog.kaggle.com/2017/08/25/data-science-101-getting-started-in-nlp-tokenization-tutorial/?utm_source=Mailing+list&utm_campaign=08fd4daab7-Kaggle_Newsletter_09-10-2017&utm_medium=email&utm_term=0_f42f9df1e1-08fd4daab7-399732741