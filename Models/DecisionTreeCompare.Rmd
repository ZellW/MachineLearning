---
title: 'Decision Tree Algo Compare'
output:
    rmdformats::readthedown:
      highlight: pygments
      code_folding: show
---
<style type="text/css">
p{ /* Normal  */
   font-size: 14px;
   line-height: 18px;
}
body{ /* Normal  */
   font-size: 14px;
}
td {  /* Table  */
   font-size: 12px;
}
h1 { /* Header 1 */
font-size: 26px;
color: #4294ce;
}
h2 { /* Header 2 */
font-size: 22px;
}
h3 { /* Header 3 */
font-size: 18px;
}
code.r{ /* Code block */
  font-size: 12px;
}
pre { /* Code block */
  font-size: 12px
}
#table-of-contents h2 {
background-color: #4294ce;
}
#table-of-contents{
background: #688FAD;
}
#nav-top span.glyphicon{
color: #4294ce;
}
#postamble{
background: #4294ce;
border-top: ;
}
</style>

```{r loadLibs1, warning=FALSE, message=FALSE}
#if(!require(bayesian_first_aid)){devtools::install_github("rasmusab/bayesian_first_aid")}
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("plyr","dplyr","ggplot2", "readr", "tidyr", "gridExtra", "stringr", "lubridate", 
        "caret", "gbm", "modelr", "ggthemes", "lightgbm", prompt = FALSE)

options(scipen = 999)#Do not display exponents

load("~/GitHub/LargeDataFiles/TreeAlgoCompare.RData")
```

# Introduction

## Step 1- Get Data

The UCI Bike dataset is available from a variety of sources.  A copy of the data has been downloaded to the local machine.  A CSV has also been saved for reproducibility.

```{r getData, eval=FALSE}
Bike <- read.csv("~/GitHub/MachineLearning/Models/data/Bike Rental UCI dataset.csv")
```

The data variables are intuitive except for `weathersit`:
 
- 1: Clear, Few clouds, Partly cloudy, Partly cloudy
- 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
- 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
- 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog

```{r glimpseBike}
glimpse(Bike)
```

## Step 2: Data pre-processing

Data pre-processing is an important step in most real-world analytic applications. The major tasks include data cleaning, data integration, data transformation, data reduction, and data discretization and quantization.

In this experiment, we used Metadata Editor and Project Columns to convert the two numeric columns "weathersit" and "season" into categorical variables and to remove four less relevant columns ("instant", "dteday", "casual", "registered").

```{r dataPreProcess, eval=FALSE}
Bike$weathersit <- as.factor(Bike$weathersit)
Bike$season <- as.factor(Bike$season)

Bike$instant <- NULL
Bike$dteday <- NULL
Bike$casual <- NULL
Bike$registered <- NULL

Bike$yr <- NULL
```

## Step 3: Feature engineering

Normally, when preparing training data you pay attention to two requirements:

- First, find the right data, integrate all relevant features and reduce the data size if necessary.
- Second, identify the features that characterize the patterns in the data and if they don't exist, construct them.

It can be tempting to includes many raw data fields in the feature set, but more often, you need to construct additional features from the raw data to provide better predictive power. This is called feature engineering.

In this experiment, the original data is augmented with a number of new columns.  3 new datasets are created with new features following the concepts detailed above.

The training datasets are based on the same raw input data, but  different additional features were added to each training set. 

- Set A = weather + holiday + weekday + weekend features for the predicted day
- Set B = number of bikes that were rented in each of the previous 12 hours
- Set C = number of bikes that were rented in each of the previous 12 days at the same hour
- Set D = number of bikes that were rented in each of the previous 12 weeks at the same hour and the same day

Each of these feature sets captures different aspects of the problem:

- Feature set B captures very recent demand for the bikes.
- Feature set C captures the demand for bikes at a particular hour. 
- Feature set D captures demand for bikes at a particular hour and particular day of the week. 

The four training datasets were built by combining the feature set as follows:

- Training set 1: feature set A only
- Training set 2: feature sets A+B
- Training set 3: feature sets A+B+C
- Training set 4: feature sets A+B+C+D

```{r commonVars, eval=FALSE}
previous_hrs <- 12
orig_names <- names(Bike)
n_rows <- dim(Bike)[1]
orig_colCnt <- dim(Bike)[2] #number of col in original data after cleaning
suffix <- -1:-previous_hrs #to create new columns
```

### Bike Demand for Last 12 Hours

Create new columns to show the bike demand from the last 12 hours.
```{r eval=FALSE}
Bike1 <- Bike

for (i in 1:previous_hrs) {
  #create new column, start at 2nd row, copy from Col 13 (cnt) - fill in 12 new columns with data from cnt
  Bike1[(i+1):n_rows, orig_colCnt+i] <- Bike1[1:(n_rows-i), orig_colCnt]
  #Fill in remaining resulting NA with the first cnt record (16)
  Bike1[1:i, orig_colCnt+i] <- Bike1[1:i, orig_colCnt+i-1]
}

new_names_hour <- paste("demand in hour", suffix)
names(Bike1) <- c(orig_names, new_names_hour)
```
```{r glimpseBike1}
glimpse(Bike1)
```

### Bike Demand Last 12 Hours at Same Hour

Add more columns.  Calculate the number of bikes that were rented in each of the previous 12 days at the same hour.

```{r eval=FALSE}
Bike2 <- Bike1
orig_colCnt2 <- orig_colCnt + previous_hrs
for (i in 1:previous_hrs) {
  Bike2[(i * 24 + 1):n_rows, orig_colCnt2 + i] <- Bike2[1:(n_rows - i * 24), orig_colCnt]
  Bike2[1:(i * 24), orig_colCnt2 + i] <- Bike2[1:(i * 24), orig_colCnt2 + i - 1] 
}

new_names_day <- paste("demand in day", suffix)
names(Bike2) <- c(orig_names, new_names_hour, new_names_day)
```
```{r glimpseBike2}
glimpse(Bike2)
```

### Bike demand in the last 12 weeks: same day and same hour

Add more columns to calculate the number of bikes that were rented in each of the previous 12 weeks at the same hour and the same day.

```{r eval=FALSE}
Bike3 <- Bike2
orig_colCnt2 <- orig_colCnt + previous_hrs * 2
for (i in 1:previous_hrs) {
  Bike3[(i * 24 * 7 + 1):n_rows, orig_colCnt2 + i] <- Bike3[1:(n_rows - i * 24 * 7), orig_colCnt]
  Bike3[1:(i * 24 * 7), orig_colCnt2 + i] <- Bike3[1:(i * 24 * 7), orig_colCnt2 + i - 1] 
}

new_names_week <- paste("demand in week", suffix)
names(Bike3) <- c(orig_names, new_names_hour, new_names_day, new_names_week)
```
```{r glimpseBike3}
glimpse(Bike3)
```

## Step 4: Train the model

Next, choose an algorithm to use in analyzing the data. There are many kinds of machine learning problems (classification, clustering, regression, recommendation, etc.) with different algorithms suited to each task, depending on their accuracy, intelligibility and efficiency.

For this experiment, because the goal was to predict a number (the demand for the bikes, represented as the number of bike rentals) we chose a regression model. Moreover, because the number of features is relatively small (less than 100) and these features are not sparse, the decision boundary is very likely to be nonlinear.

Based on these factors, a Boosted Decision Tree Regression is used, a commonly used nonlinear algorithm. However, this experiement is internded to illustrate the process a Data Scientist might use to devlop a model.  Therefore, several tree algoritms will be explored.

Rather than splitting the data, cross validation will be used.  (If we were to use train/test splits, use the `yr` variable.  In the dataset, see the column "yr" column in which 0 means 2011 and 1 means 2012. Remove `yr` since it provides no predictive power.)

```{r dataSplit, eval=FALSE, echo=FALSE}
trainData <- Bike %>% filter(yr == 0) %>% mutate(yr = NULL)
testData <- Bike %>% filter(yr ==1) %>% mutate(yr = NULL)

trainData1 <- Bike1 %>% filter(yr == 0) %>% mutate(yr = NULL)
testData1 <- Bike1 %>% filter(yr ==1) %>% mutate(yr = NULL)

trainData2 <- Bike2 %>% filter(yr == 0) %>% mutate(yr = NULL)
testData2 <- Bike2 %>% filter(yr ==1) %>% mutate(yr = NULL)

trainData3 <- Bike3 %>% filter(yr == 0) %>% mutate(yr = NULL)
testData3 <- Bike3 %>% filter(yr ==1) %>% mutate(yr = NULL)
```

## Simple Decision Tree

First, produce a common decision tree:

```{r simpleDecisionTree, eval=FALSE, echo=FALSE}
simple_tree <- rpart::rpart(cnt~., data = Bike, method = "anova", cp = 0.03)
```
```{r plotSimpleTree, eval=FALSE, echo=FALSE}
rpart.plot::rpart.plot(simple_tree)
```
```{r eval=FALSE, echo=FALSE}
#Same result but useful code for more complex models when cp (complexity parameter) is not set
#It is the amount by which splitting that node improved the relative error. For example, splitting the root node drops the relative error from 1.0 to 0.5, so the CP of the root node is 0.5. The CP of the next node is only 0.01 (which is the default limit for deciding when to consider splits). Because splitting that node resulted in an improvement of 0.01, the tree building stops.
rpart::plotcp(simple_tree)
min_cp = simple_tree$cptable[which.min(simple_tree$cptable[,"xerror"]),"CP"]
min_cp
# prune tree using best cp
simple_tree_prune = rpart::prune(simple_tree, cp = min_cp)
rpart.plot::rpart.plot(simple_tree_prune)
```

```{r eval=FALSE}
set.seed <- 12347
trainctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, verboseIter = TRUE)
#Decision Tree
#This version does not like the column names, so do not use the formula method
#rpart_tree <- train(cnt~., data = Bike1, method = "rpart", trControl = trainctrl)

rpart_tree <- train(x = Bike[, names(Bike) != "cnt"], y = Bike$cnt, method = "rpart", trControl = trainctrl)
```

```{r}
rpart.plot::rpart.plot(simple_tree)
```

## Random Forest

Compare a basic decision tree to a random forest model.  (Definitions of the models is beyond the intent of this document.)

> No graphical ouput but now we compare the RMSE of each model.

```{r rf_tree, eval=FALSE}
#Random Forest
rf_tree <- train(cnt~., data = Bike1, method = "rf",  trControl = trainctrl)
```

```{r}
resamps <- resamples(list(singleTree=rpart_tree, randomForest=rf_tree))
summary(resamps)
```

## GBM Model

Continue the evalaution of tree algoirthms using a GBM model.

```{r GBM_model, eval=FALSE}
set.seed <- 12347
gbm_tree <- train(cnt~., data = Bike1, method = "gbm", trControl = trainctrl, 
                  distribution = "gaussian")
gbm_tree
```

```{r}
resamps <- resamples(list(singleTree=rpart_tree, randomForest=rf_tree, GBM = gbm_tree))
summary(resamps)
```

```{r echo=FALSE}
getModelInfo()$gbm$parameters
```

Advance the GBM model by tuning hyperparameters.

```{r GBM_model2, eval=FALSE}
set.seed <- 12347
myGrid <- expand.grid(n.trees = c(150, 175, 200, 250),
                      interaction.depth = c(5, 6, 7, 8, 9),
                      shrinkage = c(0.075, 0.1, 0.125, 0.15, 0.2),
                      n.minobsinnode = c(7, 10, 12, 15))

gbm_tree2 <- train(cnt~., data = Bike1, method = "gbm", trControl = trainctrl, 
                  distribution = "gaussian", tuneGrid = myGrid)
gbm_tree2
```

```{r eval=FALSE, echo=FALSE}
gbm_tree2$bestTune
```

Train the GBM model using the best perforoing hyperparameters found above.

```{r eval=FALSE}
set.seed <- 12347
myGrid2 <- gbm_tree2$bestTune
gbm_tree2 <- train(cnt~., data = Bike1, method = "gbm", trControl = trainctrl, 
                  distribution = "gaussian", tuneGrid = myGrid2)
```

```{r}
resamps <- resamples(list(singleTree=rpart_tree, randomForest=rf_tree, GBM = gbm_tree, 
                          GBM_Grid = gbm_tree2))
summary(resamps)
```

```{r}
dotplot(resamps, metric = "RMSE", main = "Model Compare")
```
```{r}
bwplot(resamps, metric = "RMSE", main = "Model Compare")
```

Peek at the most influential variables in the final GBM model.

```{r}
plot(varImp(gbm_tree2, scale = TRUE))
```

```{r newVisual}
gather_residuals(Bike1, gbm_tree2, .resid = "resid", .model = "model") %>%
  ggplot(aes(cnt, resid, color = holiday)) + geom_point() 
```

## XGBoost

XGBoost is one of the most popular boosted tree algoritms.  It is commonly use in Kaggel competitons.  Lets find out how it performs on our data.

```{r message=FALSE, warning=FALSE, results='hide'}
set.seed <- 12347

library(xgboost)
library(Matrix)
#handles missing values
#requires matrix as input - all numerical values

#Partition data

myIndex <- sample(2, nrow(Bike1), replace = TRUE,  prob = c(.8, .2))

myTrain <-  Bike1[myIndex == 1,]
myTest <- Bike1[myIndex == 2,]

#Recall there are 2 factors - use one hot encoding & create matrix
# cnt is in the 12th column (names(Bike1))

#First need to remove spaces in column names - xgboost does not like these!
names(myTrain) <- gsub("\\s", "_", names(myTrain))
names(myTrain) <- gsub("-", "_", names(myTrain))

names(myTest) <- gsub("\\s", "_", names(myTest))
names(myTest) <- gsub("-", "_", names(myTest))

myTrain_m <- sparse.model.matrix(cnt ~. -cnt, data = myTrain)
myTrain_label <-  myTrain[, "cnt"]

myTrain_matrix <- xgb.DMatrix(data = as.matrix(myTrain_m), label = myTrain_label)

#Do same thing for Test data
myTest_m <- sparse.model.matrix(cnt ~. -cnt, data = myTest)
myTest_label <-  myTest[, "cnt"]

myTest_matrix <- xgb.DMatrix(data = as.matrix(myTest_m), label = myTest_label)

#Parameters
xgb_params <- list("objective" = "reg:linear", "eval_metric" = "rmse")
watchlist <- list(train = myTrain_matrix, test = myTest_matrix)

best_model <- xgb.train(params = xgb_params, data = myTrain_matrix, nrounds = 100,
                        watchlist = watchlist)
```
After 100 iterations, the final record is:

```{r}
best_model$evaluation_log[100,]

#Plot results
myErrors <- data.frame(best_model$evaluation_log)
{plot(myErrors$iter, myErrors$train_rmse, col = "blue")
lines(myErrors$iter, myErrors$test_rmse, col = "red")}
```

The "space" between the <span style="color:blue">blue</span> and <span style="color:red">red</span> lines indicate overfitting.  Ideally these lines should be close together so the results from the `train` data more closely match the results from the `test` data.

```{r eval=FALSE, echo=FALSE}
min(myErrors$test_rmse)
myErrors[myErrors$test_rmse == min(myErrors$test_rmse),]
```

Lets add a parameter below:

- lower eta is robust to overfitting.  Default = 0.3; can range 0-1

```{r results='hide'}
set.seed <- 12347
best_model <- xgb.train(params = xgb_params, data = myTrain_matrix, nrounds = 100,
                        watchlist = watchlist, eta = 0.08)
```
```{r}
best_model$evaluation_log[100,]
#Plot results
myErrors <- data.frame(best_model$evaluation_log)
{plot(myErrors$iter, myErrors$train_rmse, col = "blue")
lines(myErrors$iter, myErrors$test_rmse, col = "red")}
```

Overfitting appears to be less of a concern.

```{r XGB_Importance}
myImportance <- xgb.importance(colnames(myTrain_matrix), model = best_model)
print(myImportance)#Gain is most important

xgb.plot.importance(myImportance)
```

Run XGBoost again with more optimized parameters.

- `tree.depth`; default = 6; 1 to INF
- larger values of `gamma` produces more conservative algo (avoid overfitting); range 0 - INF; default = 0
- lower values of `subsample` helps prevent overfitting. Default = 1 (100%).  Range 0 - 1. subsample ratio of the training instance. Setting it to 0.5 means that XGBoost randomly collected half of the data instances to grow trees and this will prevent overfitting.
- `colsample_bytree` default = 1. subsample ratio of columns when constructing each tree.
- `missing` very useful when dealing with large data and much missing data

```{r XGB_MoreParams, results='hide'}
set.seed <- 12347
#http://xgboost.readthedocs.io/en/latest/parameter.html
best_model <- xgb.train(params = xgb_params, data = myTrain_matrix, nrounds = 300,
                        watchlist = watchlist, eta = 0.02, max.depth = 5,
                        gamma = 50, subsample = .5, colsample_bytree = .9,
                        missing = NA, seed = 12345)
```
```{r}
best_model$evaluation_log[300,]
#Plot results
myErrors <- data.frame(best_model$evaluation_log)
{plot(myErrors$iter, myErrors$train_rmse, col = "blue")
lines(myErrors$iter, myErrors$test_rmse, col = "red")}
```

```{r saveGBM, echo=FALSE, eval=FALSE}
save.image("~/GitHub/LargeDataFiles/TreeAlgoCompare.RData")
```

Make predictions using optimized XGBoost:

```{r XGB_Prediction}
myPredictions <- predict(best_model, newdata = myTest_matrix)

myCompare <- data.frame(Prediction = myPredictions, Actual = myTest$cnt)
head(myCompare, 20)
```

- RF uses decision trees, which are very prone to overfitting. In order to achieve higher accuracy, RF decides to create a large number of them based on bagging. The basic idea is to resample the data over and over and for each sample train a new classifier. Different classifiers overfit the data in a different way, and through voting those differences are averaged out.
- GBM is a boosting method, which builds on weak classifiers. The idea is to add a classifier at a time, so that the next classifier is trained to improve the already trained ensemble. Notice that for RF each iteration the classifier is trained independently from the rest.

# LightGBM

LightGBM is a gradient boosting framework that uses tree based learning algorithm.

*How it differs from other tree based algorithm?*

LightGBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.

*Why LightGBM is gaining extreme popularity?*

The size of data is increasing day by day and it is becoming difficult for traditional data science algorithms to give faster results. LightGBM is prefixed as ‘Light’ because of its high speed. Light GBM can handle the large size of data and takes lower memory to run. Another reason of why Light GBM is popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development.

## LightGBM Parameters
### Control Parameters

- **max_depth**: It describes the maximum depth of tree. This parameter is used to handle model overfitting. Any time you feel that your model is overfitted, my first advice will be to `lower max_depth`.

- **min_data_in_leaf**: It is the minimum number of the records a leaf may have. The default value is 20, optimum value. It is also used to deal over fitting

- **feature_fraction**: Used when your boosting(discussed later) is random forest. 0.8 feature fraction means LightGBM will select 80% of parameters randomly in each iteration for building trees.

- **bagging_fraction**: specifies the fraction of data to be used for each iteration and is generally used to speed up the training and avoid overfitting.

- **early_stopping_round**: This parameter can help you speed up your analysis. Model will stop training if one metric of one validation data doesn’t improve in last `early_stopping_round rounds`. This will reduce excessive iterations.

- **lambda**: `lambda` specifies regularization. Typical value ranges from 0 to 1.

- **min_gain_to_split**: This parameter will describe the minimum gain to make a split. It can used to control number of useful splits in tree.

- **max_cat_group**: When the number of category is large, finding the split point on it is easily over-fitting. So LightGBM merges them into `max_cat_group` groups, and finds the split points on the group boundaries, default:64

### Core Parameters

- **Task**: It specifies the task you want to perform on data. It may be either train or predict.

- **application**: This is the most important parameter and specifies the application of your model, whether it is a regression problem or classification problem. LightGBM will by default consider model as a regression model.

    - regression: for regression
    - binary: for binary classification
    - multiclass: for multiclass classification problem

- **boosting**: defines the type of algorithm you want to run, default=gdbt

    - gbdt: traditional Gradient Boosting Decision Tree
    - rf: random forest
    - dart: Dropouts meet Multiple Additive Regression Trees
    - goss: Gradient-based One-Side Sampling

- **num_boost_round**: Number of boosting iterations, typically 100+

- **learning_rate**: This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates. Typical values: 0.1, 0.001, 0.003…

- **num_leaves**: number of leaves in full tree, default: 31

- **device: default**: cpu, can also pass gpu

### Metric parameter

- **metric**: again one of the important parameter as it specifies loss for model building. Below are few general losses for regression and classification.

    - mae: mean absolute error
    - mse: mean squared error
    - binary_logloss: loss for binary classification
    - multi_logloss: loss for multi classification

### IO parameter

- **max_bin**: it denotes the maximum number of bin that feature value will bucket in.

- **categorical_feature**: It denotes the index of categorical features. If categorical_features=0,1,2 then column 0, column 1 and column 2 are categorical variables.

- **ignore_column**: same as categorical_features just instead of considering specific columns as categorical, it will completely ignore them.

- **save_binary**: If you are really dealing with the memory size of your data file then specify this parameter as ‘True’. Specifying parameter true will save the dataset to binary file, this binary file will speed your data reading time for the next time.

Knowing and using above parameters will help you implement the model. Remember, the implementation of LightGBM is easy but parameter tuning is difficult. 

```{r lightGBMinstall, eval=FALSE, echo=FALSE}
#Install CMAKE (https://cmake.org/download/) and RTools 64 bit required first!
# library(devtools)
# options(devtools.install.args = "--no-multiarch") # if you have 64-bit R only, you can skip this
# install_github("Microsoft/LightGBM", subdir = "R-package")

devtools::install_github("Microsoft/LightGBM", ref = "1b7643b", subdir = "R-package")#not certain why this works!
#See https://github.com/Microsoft/LightGBM/pull/177

#lightGBM parameters:  https://sites.google.com/view/lauraepp/parameters

# Another install attempt - FAILED
devtools::install_github("Laurae2/lgbdl")
library(lgbdl)
lgb.dl(commit = "master", compiler = "gcc", repo = "https://github.com/Microsoft/LightGBM")
```

- It is not advisable to use LGBM on small datasets. 
- Light GBM is sensitive to overfitting and can easily overfit small data. 
- There is no threshold on the number of rows but my experience suggests to use it only for data with 10,000+ rows.

## LightGBM Example

```{r lightGBM_ex, results='hide'}
datasetLGBM <- read.csv("~/GitHub/MachineLearning/Models/data/Social_Network_Ads.csv")
datasetLGBM <- datasetLGBM %>% select(-User.ID)

datasetLGBM <- bind_cols(datasetLGBM %>% select(-Purchased) %>% 
                              mlr::createDummyFeatures(), datasetLGBM %>% select(Purchased))

datasetLGBM <- datasetLGBM %>% mutate_all(as.numeric)

# PreProcess - Scaling, Center, near zero variability using caret
preObj <- preProcess(datasetLGBM[, -c(3:5)], method=c("center", "scale", "nzv"))
datasetLGBM <- predict(preObj, datasetLGBM)
#datasetLGBM[, 2:3] <- scale(datasetLGBM[,2:3])

categoricals <- datasetLGBM %>% select_if(is.factor) %>% colnames()
# proportion of data to train on
split <- 0.7
set.seed(123)
trainIndex <- caret::createDataPartition(datasetLGBM$Purchased, p = split, list = FALSE, times = 1)
dtrain <- lgb.Dataset((datasetLGBM %>% select(-Purchased) %>% data.matrix())[trainIndex,],
                     colnames = datasetLGBM %>% select(-Purchased) %>% colnames(),
                     categorical_feature = categoricals,
                     label = datasetLGBM$Purchased[trainIndex], free_raw_data=T)
dtest <- lgb.Dataset.create.valid(dtrain,
                                  (datasetLGBM %>% select(-Purchased) %>% data.matrix())[-trainIndex,],
                                  label = datasetLGBM$Purchased[-trainIndex])
params <- list(objective = "binary", metric = "binary_logloss")
valids <- list(test=dtest)
num_classes <- length(unique(datasetLGBM$Purchased))

bst <- lgb.train(params, dtrain, nrounds = 100, valids, num_leaves = 10, min_data = 50,
                boosting_type = "gbdt", sub_feature = 0.5, max_depth = 10,
                categorical_feature = categoricals)
```

Number of Rounds: `r bst$current_iter() `

Multilogloss of Best Model: `r bst$record_evals$test$binary_logloss$eval %>% unlist() %>% tail(1)`

> Entered bug on Github - lgb.importance does not appear to be included in the current code base.

```{r fig.width = 10, fig.height = 18, eval=FALSE}
df_imp <- tbl_df(lgb.importance(bst, percentage = TRUE))

df_imp %>%
  gather(Gain:Frequency,key="Metric",value="Value") %>% 
  mutate(Feature = Feature %>% to_title()) %>%
  ggplot(aes(x=reorder(Feature,Value),y=Value,fill=reorder(Feature,Value))) + 
  geom_bar(color="black",width=0.5,stat="identity",alpha=0.7) + 
  facet_wrap(~Metric,nrow=3,scales="free") + 
  scale_x_discrete("Marker") + 
  coord_flip() +
  ggtitle("LightGBM Feature Importance") +
  labs(fill = "Features") +
  theme(legend.background = element_rect(size = 0.1), 
        legend.position = "bottom",
        legend.direction = "horizontal",
        legend.title = element_text(face="bold"),
        plot.title = element_text(hjust = 0.5)) +
  theme(plot.subtitle = element_text(vjust = 1), 
    plot.caption = element_text(vjust = 1), 
    axis.text.x = element_text(vjust = 0.5, 
        angle = 90))
```

```{r lightGBM_pred_ex, eval=FALSE}
test <- (datasetLGBM %>% select(-Purchased) %>% data.matrix())[-trainIndex,]
preds_matrix <- predict(bst, test, reshape=T)
```

```{r}
datasetLGBM <- bind_cols(myTrain %>% select(-cnt) %>% 
                              mlr::createDummyFeatures(), myTrain %>% select(cnt))

datasetLGBM <- datasetLGBM %>% mutate_all(as.numeric)

# PreProcess - Scaling, Center, near zero variability using caret
#preObj <- preProcess(datasetLGBM[, -c(3:5)], method=c("center", "scale", "nzv"))
#datasetLGBM <- predict(preObj, datasetLGBM)
#datasetLGBM[, 2:3] <- scale(datasetLGBM[,2:3])

categoricals <- datasetLGBM %>% select_if(is.factor) %>% colnames()
# proportion of data to train on
split <- 0.7
set.seed(123)
trainIndex <- caret::createDataPartition(datasetLGBM$cnt, p = split, list = FALSE, times = 1)
dtrain <- lgb.Dataset((datasetLGBM %>% select(-cnt) %>% data.matrix())[trainIndex,],
                     colnames = datasetLGBM %>% select(-cnt) %>% colnames(),
                     categorical_feature = categoricals,
                     label = datasetLGBM$cnt[trainIndex], free_raw_data=T)
dtest <- lgb.Dataset.create.valid(dtrain,
                                  (datasetLGBM %>% select(-cnt) %>% data.matrix())[-trainIndex,],
                                  label = datasetLGBM$cnt[-trainIndex])
params <- list(objective = "regression", metric = "mse")
valids <- list(test=dtest)
#num_classes <- length(unique(datasetLGBM$Purchased))

bst2 <- lgb.train(params, dtrain, nrounds = 300, valids, num_leaves = 10, min_data = 50,
                boosting_type = "gbdt", sub_feature = 0.5, max_depth = 30,
                categorical_feature = categoricals)

test <- (datasetLGBM %>% select(-cnt) %>% data.matrix())[-trainIndex,]
preds_matrix <- predict(bst2, test, reshape=T)
preds_matrix[1:20]
```

# CatBoost

For another time. . . .

```{r eval=FALSE}
#install.packages('devtools')
devtools::install_github('catboost/catboost', subdir = 'catboost/R-package')
```

# Reference

Great comparision on XGBoost, LightGBM and CatBoost:

https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db

More options for decision trees than most think:

https://www.r-bloggers.com/a-brief-tour-of-the-trees-and-forests/

