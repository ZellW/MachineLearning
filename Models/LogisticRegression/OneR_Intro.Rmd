---
title: "OneR - New Baseline for Classification Models"
output:
  html_document:
    highlight: pygments
    theme: spacelab
    toc: yes
    toc_float: true
    toc_collapsed: true
    toc_depth: 3
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
setwd("~/R/Complete") #change as needed

# options
options(echo=TRUE)
options(stringsAsFactors=FALSE)


if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("tidyverse", "OneR", "kableExtra" , prompt = TRUE)
```

# Introduction

As machine learning becomes mainstream and new algorithms are developed seemingly daily, the challenge to look inside the model to understand results becomes a large problem.  Some industries require that the results for a predictive model must be understandable- they need to look into the *black box*.

There are several ways to explain model results.  In the last couple of years, tools that peek into *black boxes* have been developed.  Most notable, *lime*, *breakDown* and *xgboostExplainer*, *randomForestExplainer*, and *FFTrees* are all helpful.  However, isn't there a simpler approach?

What if we found a way to build models quickly and accurately that are easy to explain? Build "the best simple model" that balances between the best accuracy possible with a model that is still simple enough to understand.  That is the primary intent of the **OneR** package.  It is purposely build to build classification models quickly, accurately and in a manner that is simple to understand.

*I have developed the OneR package for finding this sweet spot and thereby establishing a new baseline for classification models in Machine Learning (ML).**OneR** is not a panacea - it only builds classification models.  Also, while performance is often nearly as good as more complex algorithms, the business will need to decide if the **OneR** results meet their needs.  If so, then it is a win for you because you developed the solution super-fast, and for the business to use your output to solve a business problem quickly.

# OneR Design Principles
(Note:  This content is modified from the **OneR** vignette)

- Easy: the learning curve for new users should be minimal. Results should be obtained with ease and only minimal preprocessing and modeling steps should be necessary.
- Versatile: all types of data, i.e. categorical and numeric, should be computable - as input variable as well as as target.
- Fast: the running times of model trainings should be short.
- Accurate: the accuracy of trained models should be good overall.
- Robust: models should not be prone to overfitting; the reached accuracy on training data should be comparable to the accuracy of predictions from new, unseen cases.
- Comprehensible: it should be easy to understand which rules the model has learned. 
- Reproducible: because the used algorithms are strictly deterministic one will always get the same models on the same data. (Many ML algorithms have stochastic components so that the data scientist will get a different model every time.)
- Intuitive: model diagnostics should be presented in form of simple tables and plots.
- Native R: the whole package is written in native R code. Thereby the source code can be easily checked and the whole package is very lean. Additionally the package has no dependencies at all other than base R itself.

# OneR Pseudo code

**OneR** generates one rule for each predictor in the data, then selects the rule with the smallest total error as its "one rule".  To create a rule for a predictor, construct a frequency table for each predictor against the target. It has been shown that **OneR** outputs rules *only slightly less accurate than state-of-the-art classification algorithms* while producing rules that are simple for humans to interpret.	

OneR Algorithm	

- For each predictor,
  - For each value of that predictor, make a rule as follows;
      - Count how often each value of target (class) appears
      - Find the most frequent class
      - Make the rule assign that class to this value of the predictor
  - Calculate the total error of the rules of each predictor
Choose the predictor with the smallest total error.

> OneR can be considered as a decision tree with only one split.

The code is equally simple to implement:

1. library(OneR)
2. data <- optbin(your_data)
3. model <- OneR(your.data, verbose = TRUE)
4. summary(model)
5. plot(model)
6. prediction <- predict(model, data)
7. eval_model(prediction, data)

# OneR Examples

Using **OneR** could not be simpler.  In fact it is so easy traditional programmers will think *What did I miss?  Surely there is more to it than that.*  Look out, it really is easy.  You _should_ use it to get started on any classification task.

## IRIS - Yes Again

Are you tired using the Iris dataset?  I know I am but get over it and learn how simple it is to use **OneR**.

> OneR requires categorical data so discretization is used on numerical data

Okay, I can hear you sigh.  Get over this too.  Sure, there is some concern about discretization.  **OneR** optimizes the binning operation thoughtfully.  And recall, the goal is to perform classification quickly, accurately and understandably.

Recall what the `iris` data looks like:

```{r}
kable(head(iris)) %>% kable_styling(bootstrap_options = c("striped", "condensed", "responsive"))
```

OK, the first step is discretization.  **optbin** discretizes all numerical data in a data frame into categorical bins where the cut points are optimally aligned with the target categories. A factor is returned. The cutpoints are calculated by pairwise logistic regressions (method `logreg`) or as the means of the expected values of the respective classes (`naive`). The function is likely to give unsatisfactory results when the distributions of the respective classes are not (linearly) separable. Method "naive" should only be used when distributions are (approximately) normal, although in this case `logreg` should give comparable results, so it is the preferable (and therefore default) method.

Method `infogain` is an entropy based method which calculates cut points based on information gain. The idea is that uncertainty is minimized by making the resulting bins as pure as possible. This method is the standard method of many decision tree algorithms.

```{r}
mydata <- optbin(iris)
kable(head(mydata)) %>% kable_styling(bootstrap_options = c("striped", "condensed", "responsive"))
```

Great news, now you can build the model!

```{r}
mymodel <- OneR(mydata, verbose = TRUE)
```

It is equally simple to plot the model:

```{r}
plot(mymodel)
```

Want to do a prediction and evaluate the model?  Again, very simple:

```{r}
myprediction <- predict(mymodel, mydata)
eval_model(myprediction, mydata)
```

> Very good accuracy of 96% is reached effortlessly.

"Petal.Width" is the attribute with the highest predictive value. The results are three very simple and accurate rules to predict   species.

The nearly perfect separation of the areas in the diagnostic plot give a good indication of the model's ability to separate the different species.

OK, again, I hear another sigh.  Yes, I used the same data for training and testing.  So now you do not have confidence you should use **OneR** for all classification projects - at least to get started.

## Cervical Cancer

This example uses [UCIs Cervical Cancer](https://archive.ics.uci.edu/ml/machine-learning-databases/00383/) dataset.  A cleans dataset is included in **OneR**.

The cervical cancer dataset contains indicators and risk factors for predicting if a woman will get cervical cancer. The features contain demographics (e.g. age), habits, and medical history.

Biopsy is the gold standard for diagnosing cervical cancer and is therefore the target.

```{r}
#my_url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/00383/"
#download.file(paste0(my_url, "risk_factors_cervical_cancer.csv"), destfile = "./data/cancerTrain.txt")
myRawData <- breastcancer
glimpse(myRawData)
```


```{r}
# myCancerData <- myRawData %>% mutate(Biopsy = as.character(Biopsy))
# myCancerData$Biopsy <- plyr::mapvalues(myCancerData$Biopsy, c("0", "1"), c("Benign", "Malignant"))
myRandom <- sample(1:nrow(myRawData), 0.8 * nrow(myRawData))
myData_train <- optbin(myRawData[myRandom, ], method = "infogain")
myData_test <- myData_train[-myRandom, ]
```


```{r}
mymodel_train <- OneR(myData_train, verbose = TRUE)
plot(mymodel_train)
```

```{r}
myPrediction <- predict(mymodel_train, myData_test)
my_eval <- eval_model(myPrediction, myData_test)
```

The best reported accuracy on this dataset is 95.9% and it was reached with considerable effort. Our accuracy is close to state-of-the-art! (Rerunning the code will give varying results.  Set a seed if you wish.  This is achieved with just one simple rule that when "Uniformity of Cell Size" is bigger than 2 the examined tissue is malignant. 

The separation of the areas in the diagnostic plot give a good indication of the model's ability to differentiate between benign and malignant tissue. Additionally when you look at the distribution of misclassifications not a single malignant instance is missed, which is obviously very desirable in a clinical context.

# References

https://cran.r-project.org/web/packages/OneR/vignettes/OneR.html

